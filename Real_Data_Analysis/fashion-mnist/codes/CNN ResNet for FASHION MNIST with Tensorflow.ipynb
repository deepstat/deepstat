{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN ResNet for FASHION MNIST with Tensorflow\n",
    "\n",
    "DATA SOURCE : https://www.kaggle.com/zalando-research/fashionmnist (Kaggle, Fashion MNIST)\n",
    "\n",
    "* FASHION MNIST with Python (DAY 1) : http://deepstat.tistory.com/35\n",
    "* FASHION MNIST with Python (DAY 2) : http://deepstat.tistory.com/36\n",
    "* FASHION MNIST with Python (DAY 3) : http://deepstat.tistory.com/37\n",
    "* FASHION MNIST with Python (DAY 4) : http://deepstat.tistory.com/38\n",
    "* FASHION MNIST with Python (DAY 5) : http://deepstat.tistory.com/39\n",
    "* FASHION MNIST with Python (DAY 6) : http://deepstat.tistory.com/40\n",
    "* FASHION MNIST with Python (DAY 7) : http://deepstat.tistory.com/41\n",
    "* FASHION MNIST with Python (DAY 8) : http://deepstat.tistory.com/42\n",
    "* FASHION MNIST with Python (DAY 9) : http://deepstat.tistory.com/43\n",
    "* FASHION MNIST with Python (DAY 10) : http://deepstat.tistory.com/44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing numpy, pandas, pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../datasets/fashion-mnist_train.csv\")\n",
    "data_test = pd.read_csv(\"../datasets/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train_y = data_train.label\n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x = data_train.drop(\"label\",axis=1)/256\n",
    "x_test = data_test.drop(\"label\",axis=1)/256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting valid and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "valid_idx = np.random.choice(60000,10000,replace = False)\n",
    "train_idx = list(set(range(60000))-set(valid_idx))\n",
    "\n",
    "x_train = data_train_x.iloc[train_idx,:]\n",
    "y_train = data_train_y.iloc[train_idx]\n",
    "\n",
    "x_valid = data_train_x.iloc[valid_idx,:]\n",
    "y_valid = data_train_y.iloc[valid_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Class Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class minibatchData:\n",
    "    def __init__(self, X, Y):\n",
    "        self.start_num = 0\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "\n",
    "    def minibatch(self, batch_size):\n",
    "        self.outidx = range(self.start_num,(self.start_num + batch_size))\n",
    "        self.start_num = (self.start_num + batch_size)%(self.x.shape[0])\n",
    "        return self.x.iloc[self.outidx,:], self.y.iloc[self.outidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_minibatch_data = minibatchData(x_train, y_train)\n",
    "valid_minibatch_data = minibatchData(x_valid, y_valid)\n",
    "test_minibatch_data = minibatchData(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining weight_variables and bias_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variables(shape):\n",
    "    initial = tf.random_uniform(shape=shape, minval=-.1, maxval=.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variables(shape):\n",
    "    initial = tf.random_uniform(shape=shape, minval=0, maxval=.1)\n",
    "    return tf.Variable(initial)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining conv2d and maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "def maxpool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 3, 3, 1], strides = [1, 2, 2, 1], padding = 'SAME')\n",
    "\n",
    "def avgpool(x):\n",
    "    return tf.nn.avg_pool(x, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None,784])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "y = tf.placeholder(\"int64\", [None,])\n",
    "y_dummies = tf.one_hot(y,depth = 10)\n",
    "\n",
    "drop_prob = tf.placeholder(\"float\")\n",
    "training = tf.placeholder(\"bool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_w = weight_variables([3,3,1,64])\n",
    "l1_b = bias_variables([64])\n",
    "l1_conv = conv2d(x_image, l1_w) + l1_b\n",
    "l1_relu = tf.nn.relu(l1_conv)\n",
    "l1_dropout = tf.layers.dropout(l1_relu,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_w = weight_variables([1,1,64,16])\n",
    "l2_b = bias_variables([16])\n",
    "l2_conv = conv2d(l1_dropout, l2_w) + l2_b\n",
    "l2_batch_normalization = tf.layers.batch_normalization(l2_conv)\n",
    "l2_relu = tf.nn.relu(l2_batch_normalization)\n",
    "l2_dropout = tf.layers.dropout(l2_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l3_w = weight_variables([3,3,16,16])\n",
    "l3_b = bias_variables([16])\n",
    "l3_conv = conv2d(l2_dropout, l3_w) + l3_b\n",
    "l3_batch_normalization = tf.layers.batch_normalization(l3_conv)\n",
    "l3_relu = tf.nn.relu(l3_batch_normalization)\n",
    "l3_dropout = tf.layers.dropout(l3_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l4_w = weight_variables([1,1,16,64])\n",
    "l4_b = bias_variables([64])\n",
    "l4_conv = conv2d(l3_dropout, l4_w) + l4_b\n",
    "l4_batch_normalization = tf.layers.batch_normalization(l4_conv)\n",
    "l4_dropout = tf.layers.dropout(l4_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_add = tf.nn.relu(l4_dropout + l1_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "l5_w = weight_variables([1,1,64,16])\n",
    "l5_b = bias_variables([16])\n",
    "l5_conv = conv2d(l4_add, l5_w) + l5_b\n",
    "l5_batch_normalization = tf.layers.batch_normalization(l5_conv)\n",
    "l5_relu = tf.nn.relu(l5_batch_normalization)\n",
    "l5_dropout = tf.layers.dropout(l5_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l6_w = weight_variables([3,3,16,16])\n",
    "l6_b = bias_variables([16])\n",
    "l6_conv = conv2d(l5_dropout, l6_w) + l6_b\n",
    "l6_batch_normalization = tf.layers.batch_normalization(l6_conv)\n",
    "l6_relu = tf.nn.relu(l6_batch_normalization)\n",
    "l6_dropout = tf.layers.dropout(l6_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l7_w = weight_variables([1,1,16,64])\n",
    "l7_b = bias_variables([64])\n",
    "l7_conv = conv2d(l6_dropout, l7_w) + l7_b\n",
    "l7_batch_normalization = tf.layers.batch_normalization(l7_conv)\n",
    "l7_dropout = tf.layers.dropout(l7_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l7_add = tf.nn.relu(l7_dropout + l4_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l8_w = weight_variables([1,1,64,16])\n",
    "l8_b = bias_variables([16])\n",
    "l8_conv = conv2d(l7_add, l8_w) + l8_b\n",
    "l8_batch_normalization = tf.layers.batch_normalization(l8_conv)\n",
    "l8_relu = tf.nn.relu(l8_batch_normalization)\n",
    "l8_dropout = tf.layers.dropout(l8_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l9_w = weight_variables([3,3,16,16])\n",
    "l9_b = bias_variables([16])\n",
    "l9_conv = conv2d(l8_dropout, l9_w) + l9_b\n",
    "l9_batch_normalization = tf.layers.batch_normalization(l9_conv)\n",
    "l9_relu = tf.nn.relu(l9_batch_normalization)\n",
    "l9_dropout = tf.layers.dropout(l9_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l10_w = weight_variables([1,1,16,64])\n",
    "l10_b = bias_variables([64])\n",
    "l10_conv = conv2d(l9_dropout, l10_w) + l10_b\n",
    "l10_batch_normalization = tf.layers.batch_normalization(l10_conv)\n",
    "l10_dropout = tf.layers.dropout(l10_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l10_add = tf.nn.relu(l10_dropout + l7_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l11_w = weight_variables([1,1,64,16])\n",
    "l11_b = bias_variables([16])\n",
    "l11_conv = conv2d(l10_add, l11_w) + l11_b\n",
    "l11_batch_normalization = tf.layers.batch_normalization(l11_conv)\n",
    "l11_relu = tf.nn.relu(l11_batch_normalization)\n",
    "l11_dropout = tf.layers.dropout(l11_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l12_w = weight_variables([3,3,16,16])\n",
    "l12_b = bias_variables([16])\n",
    "l12_conv = conv2d(l11_dropout, l12_w) + l12_b\n",
    "l12_batch_normalization = tf.layers.batch_normalization(l12_conv)\n",
    "l12_relu = tf.nn.relu(l12_batch_normalization)\n",
    "l12_dropout = tf.layers.dropout(l12_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l13_w = weight_variables([1,1,16,64])\n",
    "l13_b = bias_variables([64])\n",
    "l13_conv = conv2d(l12_dropout, l13_w) + l13_b\n",
    "l13_batch_normalization = tf.layers.batch_normalization(l13_conv)\n",
    "l13_dropout = tf.layers.dropout(l13_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "l13_add = tf.nn.relu(l13_dropout + l10_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "l14_w = weight_variables([1,1,64,16])\n",
    "l14_b = bias_variables([16])\n",
    "l14_conv = conv2d(l13_add, l14_w) + l14_b\n",
    "l14_batch_normalization = tf.layers.batch_normalization(l14_conv)\n",
    "l14_relu = tf.nn.relu(l14_batch_normalization)\n",
    "l14_dropout = tf.layers.dropout(l14_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l15_w = weight_variables([3,3,16,16])\n",
    "l15_b = bias_variables([16])\n",
    "l15_conv = conv2d(l14_dropout, l15_w) + l15_b\n",
    "l15_batch_normalization = tf.layers.batch_normalization(l15_conv)\n",
    "l15_relu = tf.nn.relu(l15_batch_normalization)\n",
    "l15_dropout = tf.layers.dropout(l15_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l16_w = weight_variables([1,1,16,64])\n",
    "l16_b = bias_variables([64])\n",
    "l16_conv = conv2d(l15_dropout, l16_w) + l16_b\n",
    "l16_batch_normalization = tf.layers.batch_normalization(l16_conv)\n",
    "l16_dropout = tf.layers.dropout(l16_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "l16_add = tf.nn.relu(l16_dropout + l13_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "l17_w = weight_variables([1,1,64,32])\n",
    "l17_b = bias_variables([32])\n",
    "l17_conv = conv2d(l16_add, l17_w) + l17_b\n",
    "l17_batch_normalization = tf.layers.batch_normalization(l17_conv)\n",
    "l17_relu = tf.nn.relu(l17_batch_normalization)\n",
    "l17_avgpool = avgpool(l17_relu)\n",
    "l17_dropout = tf.layers.dropout(l17_avgpool, rate = drop_prob, training = training)\n",
    "\n",
    "l18_w = weight_variables([3,3,32,32])\n",
    "l18_b = bias_variables([32])\n",
    "l18_conv = conv2d(l17_dropout, l18_w) + l18_b\n",
    "l18_batch_normalization = tf.layers.batch_normalization(l18_conv)\n",
    "l18_relu = tf.nn.relu(l18_batch_normalization)\n",
    "l18_dropout = tf.layers.dropout(l18_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l19_w = weight_variables([1,1,32,128])\n",
    "l19_b = bias_variables([128])\n",
    "l19_conv = conv2d(l18_dropout, l19_w) + l19_b\n",
    "l19_batch_normalization = tf.layers.batch_normalization(l19_conv)\n",
    "l19_dropout = tf.layers.dropout(l19_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "l16_w2 = weight_variables([1,1,64,128])\n",
    "l16_conv2 = conv2d(l16_add, l16_w2)\n",
    "l16_add2 = avgpool(l16_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l19_add = tf.nn.relu(l19_dropout + l16_add2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "l20_w = weight_variables([1,1,128,32])\n",
    "l20_b = bias_variables([32])\n",
    "l20_conv = conv2d(l19_add, l20_w) + l20_b\n",
    "l20_batch_normalization = tf.layers.batch_normalization(l20_conv)\n",
    "l20_relu = tf.nn.relu(l20_batch_normalization)\n",
    "l20_dropout = tf.layers.dropout(l20_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l21_w = weight_variables([3,3,32,32])\n",
    "l21_b = bias_variables([32])\n",
    "l21_conv = conv2d(l20_dropout, l21_w) + l21_b\n",
    "l21_batch_normalization = tf.layers.batch_normalization(l21_conv)\n",
    "l21_relu = tf.nn.relu(l21_batch_normalization)\n",
    "l21_dropout = tf.layers.dropout(l21_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l22_w = weight_variables([1,1,32,128])\n",
    "l22_b = bias_variables([128])\n",
    "l22_conv = conv2d(l21_dropout, l22_w) + l22_b\n",
    "l22_batch_normalization = tf.layers.batch_normalization(l22_conv)\n",
    "l22_dropout = tf.layers.dropout(l22_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "l22_add = tf.nn.relu(l22_dropout + l19_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "l23_w = weight_variables([1,1,128,32])\n",
    "l23_b = bias_variables([32])\n",
    "l23_conv = conv2d(l22_add, l23_w) + l23_b\n",
    "l23_batch_normalization = tf.layers.batch_normalization(l23_conv)\n",
    "l23_relu = tf.nn.relu(l23_batch_normalization)\n",
    "l23_dropout = tf.layers.dropout(l23_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l24_w = weight_variables([3,3,32,32])\n",
    "l24_b = bias_variables([32])\n",
    "l24_conv = conv2d(l23_dropout, l24_w) + l24_b\n",
    "l24_batch_normalization = tf.layers.batch_normalization(l24_conv)\n",
    "l24_relu = tf.nn.relu(l24_batch_normalization)\n",
    "l24_dropout = tf.layers.dropout(l24_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l25_w = weight_variables([1,1,32,128])\n",
    "l25_b = bias_variables([128])\n",
    "l25_conv = conv2d(l24_dropout, l25_w) + l25_b\n",
    "l25_batch_normalization = tf.layers.batch_normalization(l25_conv)\n",
    "l25_dropout = tf.layers.dropout(l25_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "l25_add = tf.nn.relu(l25_dropout + l22_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "l26_w = weight_variables([1,1,128,32])\n",
    "l26_b = bias_variables([32])\n",
    "l26_conv = conv2d(l25_add, l26_w) + l26_b\n",
    "l26_batch_normalization = tf.layers.batch_normalization(l26_conv)\n",
    "l26_relu = tf.nn.relu(l26_batch_normalization)\n",
    "l26_dropout = tf.layers.dropout(l26_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l27_w = weight_variables([3,3,32,32])\n",
    "l27_b = bias_variables([32])\n",
    "l27_conv = conv2d(l26_dropout, l27_w) + l27_b\n",
    "l27_batch_normalization = tf.layers.batch_normalization(l27_conv)\n",
    "l27_relu = tf.nn.relu(l27_batch_normalization)\n",
    "l27_dropout = tf.layers.dropout(l27_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l28_w = weight_variables([1,1,32,128])\n",
    "l28_b = bias_variables([128])\n",
    "l28_conv = conv2d(l27_dropout, l28_w) + l28_b\n",
    "l28_batch_normalization = tf.layers.batch_normalization(l28_conv)\n",
    "l28_dropout = tf.layers.dropout(l28_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "l28_add = tf.nn.relu(l28_dropout + l25_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "l29_w = weight_variables([1,1,128,32])\n",
    "l29_b = bias_variables([32])\n",
    "l29_conv = conv2d(l28_add, l29_w) + l29_b\n",
    "l29_batch_normalization = tf.layers.batch_normalization(l29_conv)\n",
    "l29_relu = tf.nn.relu(l29_batch_normalization)\n",
    "l29_dropout = tf.layers.dropout(l29_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l30_w = weight_variables([3,3,32,32])\n",
    "l30_b = bias_variables([32])\n",
    "l30_conv = conv2d(l29_dropout, l30_w) + l30_b\n",
    "l30_batch_normalization = tf.layers.batch_normalization(l30_conv)\n",
    "l30_relu = tf.nn.relu(l30_batch_normalization)\n",
    "l30_dropout = tf.layers.dropout(l30_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l31_w = weight_variables([1,1,32,128])\n",
    "l31_b = bias_variables([128])\n",
    "l31_conv = conv2d(l30_dropout, l31_w) + l31_b\n",
    "l31_batch_normalization = tf.layers.batch_normalization(l31_conv)\n",
    "l31_dropout = tf.layers.dropout(l31_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "l31_add = tf.nn.relu(l31_dropout + l28_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "l32_w = weight_variables([1,1,128,32])\n",
    "l32_b = bias_variables([32])\n",
    "l32_conv = conv2d(l31_add, l32_w) + l32_b\n",
    "l32_batch_normalization = tf.layers.batch_normalization(l32_conv)\n",
    "l32_relu = tf.nn.relu(l32_batch_normalization)\n",
    "l32_dropout = tf.layers.dropout(l32_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l33_w = weight_variables([3,3,32,32])\n",
    "l33_b = bias_variables([32])\n",
    "l33_conv = conv2d(l32_dropout, l33_w) + l33_b\n",
    "l33_batch_normalization = tf.layers.batch_normalization(l33_conv)\n",
    "l33_relu = tf.nn.relu(l33_batch_normalization)\n",
    "l33_dropout = tf.layers.dropout(l33_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l34_w = weight_variables([1,1,32,128])\n",
    "l34_b = bias_variables([128])\n",
    "l34_conv = conv2d(l33_dropout, l34_w) + l34_b\n",
    "l34_batch_normalization = tf.layers.batch_normalization(l34_conv)\n",
    "l34_dropout = tf.layers.dropout(l34_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "l34_add = tf.nn.relu(l34_dropout + l31_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "l35_w = weight_variables([1,1,128,64])\n",
    "l35_b = bias_variables([64])\n",
    "l35_conv = conv2d(l34_add, l35_w) + l35_b\n",
    "l35_batch_normalization = tf.layers.batch_normalization(l35_conv)\n",
    "l35_relu = tf.nn.relu(l35_batch_normalization)\n",
    "l35_avgpool = avgpool(l35_relu)\n",
    "l35_dropout = tf.layers.dropout(l35_avgpool, rate = drop_prob, training = training)\n",
    "\n",
    "l36_w = weight_variables([3,3,64,64])\n",
    "l36_b = bias_variables([64])\n",
    "l36_conv = conv2d(l35_dropout, l36_w) + l36_b\n",
    "l36_batch_normalization = tf.layers.batch_normalization(l36_conv)\n",
    "l36_relu = tf.nn.relu(l36_batch_normalization)\n",
    "l36_dropout = tf.layers.dropout(l36_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l37_w = weight_variables([1,1,64,256])\n",
    "l37_b = bias_variables([256])\n",
    "l37_conv = conv2d(l36_dropout, l37_w) + l37_b\n",
    "l37_batch_normalization = tf.layers.batch_normalization(l37_conv)\n",
    "l37_dropout = tf.layers.dropout(l37_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "l34_w2 = weight_variables([1,1,128,256])\n",
    "l34_conv2 = conv2d(l34_add, l34_w2)\n",
    "l34_add2 = avgpool(l34_conv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "l37_add = tf.nn.relu(l37_dropout + l34_add2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "l38_w = weight_variables([1,1,256,64])\n",
    "l38_b = bias_variables([64])\n",
    "l38_conv = conv2d(l37_add, l38_w) + l38_b\n",
    "l38_batch_normalization = tf.layers.batch_normalization(l38_conv)\n",
    "l38_relu = tf.nn.relu(l38_batch_normalization)\n",
    "l38_dropout = tf.layers.dropout(l38_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l39_w = weight_variables([3,3,64,64])\n",
    "l39_b = bias_variables([64])\n",
    "l39_conv = conv2d(l38_dropout, l39_w) + l39_b\n",
    "l39_batch_normalization = tf.layers.batch_normalization(l39_conv)\n",
    "l39_relu = tf.nn.relu(l39_batch_normalization)\n",
    "l39_dropout = tf.layers.dropout(l39_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l40_w = weight_variables([1,1,64,256])\n",
    "l40_b = bias_variables([256])\n",
    "l40_conv = conv2d(l39_dropout, l40_w) + l40_b\n",
    "l40_batch_normalization = tf.layers.batch_normalization(l40_conv)\n",
    "l40_dropout = tf.layers.dropout(l40_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "l40_add = tf.nn.relu(l40_dropout + l37_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "l41_w = weight_variables([1,1,256,64])\n",
    "l41_b = bias_variables([64])\n",
    "l41_conv = conv2d(l40_add, l41_w) + l41_b\n",
    "l41_batch_normalization = tf.layers.batch_normalization(l41_conv)\n",
    "l41_relu = tf.nn.relu(l41_batch_normalization)\n",
    "l41_dropout = tf.layers.dropout(l41_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l42_w = weight_variables([3,3,64,64])\n",
    "l42_b = bias_variables([64])\n",
    "l42_conv = conv2d(l41_dropout, l42_w) + l42_b\n",
    "l42_batch_normalization = tf.layers.batch_normalization(l42_conv)\n",
    "l42_relu = tf.nn.relu(l42_batch_normalization)\n",
    "l42_dropout = tf.layers.dropout(l42_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l43_w = weight_variables([1,1,64,256])\n",
    "l43_b = bias_variables([256])\n",
    "l43_conv = conv2d(l42_dropout, l43_w) + l43_b\n",
    "l43_batch_normalization = tf.layers.batch_normalization(l43_conv)\n",
    "l43_dropout = tf.layers.dropout(l43_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "l43_add = tf.nn.relu(l43_dropout + l40_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "l44_w = weight_variables([1,1,256,64])\n",
    "l44_b = bias_variables([64])\n",
    "l44_conv = conv2d(l43_add, l44_w) + l44_b\n",
    "l44_batch_normalization = tf.layers.batch_normalization(l44_conv)\n",
    "l44_relu = tf.nn.relu(l44_batch_normalization)\n",
    "l44_dropout = tf.layers.dropout(l44_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l45_w = weight_variables([3,3,64,64])\n",
    "l45_b = bias_variables([64])\n",
    "l45_conv = conv2d(l44_dropout, l45_w) + l45_b\n",
    "l45_batch_normalization = tf.layers.batch_normalization(l45_conv)\n",
    "l45_relu = tf.nn.relu(l45_batch_normalization)\n",
    "l45_dropout = tf.layers.dropout(l45_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l46_w = weight_variables([1,1,64,256])\n",
    "l46_b = bias_variables([256])\n",
    "l46_conv = conv2d(l45_dropout, l46_w) + l46_b\n",
    "l46_batch_normalization = tf.layers.batch_normalization(l46_conv)\n",
    "l46_dropout = tf.layers.dropout(l46_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l46_add = tf.nn.relu(l46_dropout + l43_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "l47_w = weight_variables([1,1,256,64])\n",
    "l47_b = bias_variables([64])\n",
    "l47_conv = conv2d(l46_add, l47_w) + l47_b\n",
    "l47_batch_normalization = tf.layers.batch_normalization(l47_conv)\n",
    "l47_relu = tf.nn.relu(l47_batch_normalization)\n",
    "l47_dropout = tf.layers.dropout(l47_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l48_w = weight_variables([3,3,64,64])\n",
    "l48_b = bias_variables([64])\n",
    "l48_conv = conv2d(l47_dropout, l48_w) + l48_b\n",
    "l48_batch_normalization = tf.layers.batch_normalization(l48_conv)\n",
    "l48_relu = tf.nn.relu(l48_batch_normalization)\n",
    "l48_dropout = tf.layers.dropout(l48_relu, rate = drop_prob, training = training)\n",
    "\n",
    "l49_w = weight_variables([1,1,64,256])\n",
    "l49_b = bias_variables([256])\n",
    "l49_conv = conv2d(l48_dropout, l49_w) + l49_b\n",
    "l49_batch_normalization = tf.layers.batch_normalization(l49_conv)\n",
    "l49_dropout = tf.layers.dropout(l49_batch_normalization, rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "l49_add = tf.nn.relu(l49_dropout + l46_add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "l50_avgpool = tf.nn.avg_pool(l49_add, ksize=[1, 8, 8, 1], strides = [1, 8, 8, 1], padding = 'SAME')\n",
    "l50_flatten = tf.reshape(l50_avgpool, [-1,256])\n",
    "l50_batch_normalization = tf.layers.batch_normalization(l50_flatten)\n",
    "l50_w = weight_variables([256,10])\n",
    "l50_b = bias_variables([10])\n",
    "l50_inner_product = tf.matmul(l50_batch_normalization, l50_w) + l50_b\n",
    "l50_log_softmax = tf.nn.log_softmax(l50_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_loss = -tf.reduce_mean( tf.multiply(y_dummies,l50_log_softmax) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = tf.argmax(l50_log_softmax,axis=1)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(y, pred_labels),\"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.placeholder(\"float\")\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(xent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP : 0/16  epoch : 0  max_valid_acc = 0.7916999989748001\n",
      "DP : 0/16  epoch : 0 -- training cross-entropy : 0.12575874673202633\n",
      "DP : 0/16  epoch : 0 training_acc = 0.7891799974441528 valid_acc = 0.7916999989748001\n",
      "DP : 0/16  epoch : 1  max_valid_acc = 0.8494000029563904\n",
      "DP : 0/16  epoch : 2  max_valid_acc = 0.872299998998642\n",
      "DP : 0/16  epoch : 3  max_valid_acc = 0.8844000023603439\n",
      "DP : 0/16  epoch : 4  max_valid_acc = 0.885100000500679\n",
      "DP : 0/16  epoch : 5  max_valid_acc = 0.891700000166893\n",
      "DP : 0/16  epoch : 6  max_valid_acc = 0.8987000018358231\n",
      "DP : 0/16  epoch : 7  max_valid_acc = 0.9010999983549118\n",
      "DP : 0/16  epoch : 8  max_valid_acc = 0.9029000002145767\n",
      "DP : 0/16  epoch : 9  max_valid_acc = 0.9045000016689301\n",
      "DP : 0/16  epoch : 12  max_valid_acc = 0.904600003361702\n",
      "DP : 0/16  epoch : 14  max_valid_acc = 0.9053000032901763\n",
      "DP : 0/16  epoch : 15  max_valid_acc = 0.9059000033140182\n",
      "DP : 0/16  epoch : 26  max_valid_acc = 0.906299998164177\n",
      "DP : 0/16  epoch : 32  max_valid_acc = 0.9066000008583068\n",
      "DP : 0/16  epoch : 40  max_valid_acc = 0.9074000036716461\n",
      "DP : 0/16  epoch : 43  max_valid_acc = 0.9091000014543533\n",
      "DP : 0/16  epoch : 49  max_valid_acc = 0.910900000333786\n",
      "DP : 0/16  epoch : 50  max_valid_acc = 0.9128999996185303\n",
      "DP : 0/16  epoch : 50 -- training cross-entropy : 0.004431415451021167\n",
      "DP : 0/16  epoch : 50 training_acc = 0.9788800086975098 valid_acc = 0.9128999996185303\n",
      "DP : 0/16  epoch : 58  max_valid_acc = 0.914099999666214\n",
      "DP : 0/16  epoch : 73  max_valid_acc = 0.9145000022649765\n",
      "DP : 0/16  epoch : 84  max_valid_acc = 0.9160000002384185\n",
      "DP : 0/16  epoch : 85  max_valid_acc = 0.9187000024318696\n",
      "DP : 0/16  epoch : 100 -- training cross-entropy : 0.00011522334130108903\n",
      "DP : 0/16  epoch : 100 training_acc = 0.9997200002670288 valid_acc = 0.9162999993562698\n",
      "DP : 0/16  epoch : 105  max_valid_acc = 0.9188999998569488\n",
      "DP : 0/16  converged  epoch : 109\n",
      "DP : 1/16  epoch : 0  max_valid_acc = 0.7295999962091446\n",
      "DP : 1/16  epoch : 0 -- training cross-entropy : 0.20441408503055572\n",
      "DP : 1/16  epoch : 0 training_acc = 0.726699999332428 valid_acc = 0.7295999962091446\n",
      "DP : 1/16  epoch : 1  max_valid_acc = 0.8348000013828277\n",
      "DP : 1/16  epoch : 2  max_valid_acc = 0.866800000667572\n",
      "DP : 1/16  epoch : 3  max_valid_acc = 0.8779000008106231\n",
      "DP : 1/16  epoch : 4  max_valid_acc = 0.8793999999761581\n",
      "DP : 1/16  epoch : 5  max_valid_acc = 0.8825000005960465\n",
      "DP : 1/16  epoch : 6  max_valid_acc = 0.8902000015974045\n",
      "DP : 1/16  epoch : 7  max_valid_acc = 0.8936000019311905\n",
      "DP : 1/16  epoch : 8  max_valid_acc = 0.8958000046014786\n",
      "DP : 1/16  epoch : 9  max_valid_acc = 0.9046000015735626\n",
      "DP : 1/16  epoch : 10  max_valid_acc = 0.9080000025033951\n",
      "DP : 1/16  epoch : 11  max_valid_acc = 0.9148000025749207\n",
      "DP : 1/16  epoch : 13  max_valid_acc = 0.9166000020503998\n",
      "DP : 1/16  epoch : 15  max_valid_acc = 0.9175000047683716\n",
      "DP : 1/16  epoch : 16  max_valid_acc = 0.9205000007152557\n",
      "DP : 1/16  epoch : 24  max_valid_acc = 0.9221000021696091\n",
      "DP : 1/16  epoch : 29  max_valid_acc = 0.9232000035047531\n",
      "DP : 1/16  epoch : 35  max_valid_acc = 0.9237000060081482\n",
      "DP : 1/16  epoch : 50 -- training cross-entropy : 0.011930404643062503\n",
      "DP : 1/16  epoch : 50 training_acc = 0.972260008096695 valid_acc = 0.9214000064134598\n",
      "DP : 1/16  epoch : 71  max_valid_acc = 0.9244000017642975\n",
      "DP : 1/16  epoch : 81  max_valid_acc = 0.924500002861023\n",
      "DP : 1/16  epoch : 85  max_valid_acc = 0.9248000025749207\n",
      "DP : 1/16  epoch : 98  max_valid_acc = 0.9257000017166138\n",
      "DP : 1/16  epoch : 100 -- training cross-entropy : 0.004851734852476511\n",
      "DP : 1/16  epoch : 100 training_acc = 0.9924600064754486 valid_acc = 0.9239000034332275\n",
      "DP : 1/16  epoch : 140  max_valid_acc = 0.9262000036239624\n",
      "DP : 1/16  epoch : 150 -- training cross-entropy : 0.0014326626591846434\n",
      "DP : 1/16  epoch : 150 training_acc = 0.9998400001525879 valid_acc = 0.9248000019788742\n",
      "DP : 1/16  epoch : 153  max_valid_acc = 0.9267000031471252\n",
      "DP : 1/16  epoch : 155  max_valid_acc = 0.9275000029802323\n",
      "DP : 1/16  epoch : 180  max_valid_acc = 0.9278000009059906\n",
      "DP : 1/16  epoch : 187  max_valid_acc = 0.9287000012397766\n",
      "DP : 1/16  epoch : 200 -- training cross-entropy : 0.0007063996306642366\n",
      "DP : 1/16  epoch : 200 training_acc = 1.0 valid_acc = 0.927299998998642\n",
      "DP : 1/16  epoch : 210  max_valid_acc = 0.9291000020503998\n",
      "DP : 1/16  epoch : 211  max_valid_acc = 0.9296000015735626\n",
      "DP : 1/16  epoch : 250 -- training cross-entropy : 0.000595183692192677\n",
      "DP : 1/16  epoch : 250 training_acc = 1.0 valid_acc = 0.9275000029802323\n",
      "DP : 1/16  converged  epoch : 262\n",
      "DP : 2/16  epoch : 0  max_valid_acc = 0.7591000002622604\n",
      "DP : 2/16  epoch : 0 -- training cross-entropy : 0.14728072889149188\n",
      "DP : 2/16  epoch : 0 training_acc = 0.7627599992752075 valid_acc = 0.7591000002622604\n",
      "DP : 2/16  epoch : 1  max_valid_acc = 0.8465000021457673\n",
      "DP : 2/16  epoch : 2  max_valid_acc = 0.8625999999046325\n",
      "DP : 2/16  epoch : 3  max_valid_acc = 0.865599998831749\n",
      "DP : 2/16  epoch : 4  max_valid_acc = 0.8682000011205673\n",
      "DP : 2/16  epoch : 5  max_valid_acc = 0.8777999991178512\n",
      "DP : 2/16  epoch : 6  max_valid_acc = 0.8837999987602234\n",
      "DP : 2/16  epoch : 7  max_valid_acc = 0.890600004196167\n",
      "DP : 2/16  epoch : 8  max_valid_acc = 0.9035000032186509\n",
      "DP : 2/16  epoch : 9  max_valid_acc = 0.9036000007390976\n",
      "DP : 2/16  epoch : 10  max_valid_acc = 0.9070000004768372\n",
      "DP : 2/16  epoch : 12  max_valid_acc = 0.9143000024557114\n",
      "DP : 2/16  epoch : 16  max_valid_acc = 0.91730000436306\n",
      "DP : 2/16  epoch : 19  max_valid_acc = 0.9216000026464463\n",
      "DP : 2/16  epoch : 23  max_valid_acc = 0.9249000012874603\n",
      "DP : 2/16  epoch : 29  max_valid_acc = 0.9254000037908554\n",
      "DP : 2/16  epoch : 38  max_valid_acc = 0.9261000025272369\n",
      "DP : 2/16  epoch : 44  max_valid_acc = 0.9272000008821487\n",
      "DP : 2/16  epoch : 48  max_valid_acc = 0.9281000006198883\n",
      "DP : 2/16  epoch : 50  max_valid_acc = 0.928200004696846\n",
      "DP : 2/16  epoch : 50 -- training cross-entropy : 0.015543020361103117\n",
      "DP : 2/16  epoch : 50 training_acc = 0.9619000039100647 valid_acc = 0.928200004696846\n",
      "DP : 2/16  epoch : 60  max_valid_acc = 0.9286000019311905\n",
      "DP : 2/16  epoch : 61  max_valid_acc = 0.9291000044345856\n",
      "DP : 2/16  epoch : 62  max_valid_acc = 0.9306000018119812\n",
      "DP : 2/16  epoch : 64  max_valid_acc = 0.9331999987363815\n",
      "DP : 2/16  epoch : 85  max_valid_acc = 0.9334000015258789\n",
      "DP : 2/16  epoch : 100 -- training cross-entropy : 0.008087825570837594\n",
      "DP : 2/16  epoch : 100 training_acc = 0.9890200090408325 valid_acc = 0.9285000056028366\n",
      "DP : 2/16  epoch : 101  max_valid_acc = 0.9336000019311905\n",
      "DP : 2/16  epoch : 150 -- training cross-entropy : 0.004328535903405282\n",
      "DP : 2/16  epoch : 150 training_acc = 0.9974000024795532 valid_acc = 0.9282000011205673\n",
      "DP : 2/16  epoch : 200 -- training cross-entropy : 0.0028928844772526646\n",
      "DP : 2/16  epoch : 200 training_acc = 0.9997800002098084 valid_acc = 0.9317000025510788\n",
      "DP : 2/16  epoch : 250 -- training cross-entropy : 0.0023149583506019552\n",
      "DP : 2/16  epoch : 250 training_acc = 0.9998800001144409 valid_acc = 0.9311000019311905\n",
      "DP : 2/16  converged  epoch : 252\n",
      "DP : 3/16  epoch : 0  max_valid_acc = 0.7703999954462052\n",
      "DP : 3/16  epoch : 0 -- training cross-entropy : 0.12833979573100807\n",
      "DP : 3/16  epoch : 0 training_acc = 0.7724599986076355 valid_acc = 0.7703999954462052\n",
      "DP : 3/16  epoch : 1  max_valid_acc = 0.846499999165535\n",
      "DP : 3/16  epoch : 2  max_valid_acc = 0.8604000002145767\n",
      "DP : 3/16  epoch : 3  max_valid_acc = 0.8639999967813492\n",
      "DP : 3/16  epoch : 4  max_valid_acc = 0.8712999999523163\n",
      "DP : 3/16  epoch : 6  max_valid_acc = 0.8738000023365021\n",
      "DP : 3/16  epoch : 7  max_valid_acc = 0.8809000027179718\n",
      "DP : 3/16  epoch : 8  max_valid_acc = 0.8850000017881393\n",
      "DP : 3/16  epoch : 9  max_valid_acc = 0.8929999989271163\n",
      "DP : 3/16  epoch : 10  max_valid_acc = 0.8976999974250793\n",
      "DP : 3/16  epoch : 11  max_valid_acc = 0.8986000007390976\n",
      "DP : 3/16  epoch : 12  max_valid_acc = 0.8998999989032745\n",
      "DP : 3/16  epoch : 13  max_valid_acc = 0.9036999988555908\n",
      "DP : 3/16  epoch : 14  max_valid_acc = 0.9060000038146973\n",
      "DP : 3/16  epoch : 15  max_valid_acc = 0.9072000020742417\n",
      "DP : 3/16  epoch : 16  max_valid_acc = 0.9107000029087067\n",
      "DP : 3/16  epoch : 18  max_valid_acc = 0.9111000007390976\n",
      "DP : 3/16  epoch : 19  max_valid_acc = 0.9135000002384186\n",
      "DP : 3/16  epoch : 20  max_valid_acc = 0.9139000016450882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DP : 3/16  epoch : 21  max_valid_acc = 0.916900002360344\n",
      "DP : 3/16  epoch : 24  max_valid_acc = 0.91780000269413\n",
      "DP : 3/16  epoch : 26  max_valid_acc = 0.9193000012636184\n",
      "DP : 3/16  epoch : 27  max_valid_acc = 0.9195000022649765\n",
      "DP : 3/16  epoch : 30  max_valid_acc = 0.9196000015735626\n",
      "DP : 3/16  epoch : 33  max_valid_acc = 0.9214000034332276\n",
      "DP : 3/16  epoch : 34  max_valid_acc = 0.9225000005960464\n",
      "DP : 3/16  epoch : 35  max_valid_acc = 0.9237999999523163\n",
      "DP : 3/16  epoch : 42  max_valid_acc = 0.9238000017404556\n",
      "DP : 3/16  epoch : 45  max_valid_acc = 0.9263999998569489\n",
      "DP : 3/16  epoch : 50 -- training cross-entropy : 0.018829643983393908\n",
      "DP : 3/16  epoch : 50 training_acc = 0.9460800011157989 valid_acc = 0.9227000010013581\n",
      "DP : 3/16  epoch : 56  max_valid_acc = 0.9267000049352646\n",
      "DP : 3/16  epoch : 57  max_valid_acc = 0.928700003027916\n",
      "DP : 3/16  epoch : 60  max_valid_acc = 0.9291000008583069\n",
      "DP : 3/16  epoch : 61  max_valid_acc = 0.9308000022172928\n",
      "DP : 3/16  epoch : 87  max_valid_acc = 0.9325000017881393\n",
      "DP : 3/16  epoch : 100 -- training cross-entropy : 0.012063768751919269\n",
      "DP : 3/16  epoch : 100 training_acc = 0.9736400086879731 valid_acc = 0.9295000034570694\n",
      "DP : 3/16  epoch : 150 -- training cross-entropy : 0.008541674865409733\n",
      "DP : 3/16  epoch : 150 training_acc = 0.9888200094699859 valid_acc = 0.9303000050783158\n",
      "DP : 3/16  epoch : 162  max_valid_acc = 0.9326000028848648\n",
      "DP : 3/16  epoch : 175  max_valid_acc = 0.9329000031948089\n",
      "DP : 3/16  epoch : 194  max_valid_acc = 0.9338000029325485\n",
      "DP : 3/16  epoch : 200 -- training cross-entropy : 0.0067189472580794244\n",
      "DP : 3/16  epoch : 200 training_acc = 0.9937800056934357 valid_acc = 0.9311000031232833\n",
      "DP : 3/16  epoch : 250 -- training cross-entropy : 0.006442885615630075\n",
      "DP : 3/16  epoch : 250 training_acc = 0.9942400051355362 valid_acc = 0.9314000046253205\n",
      "DP : 3/16  epoch : 300 -- training cross-entropy : 0.0065578894598875195\n",
      "DP : 3/16  epoch : 300 training_acc = 0.9944000052213668 valid_acc = 0.9314000046253205\n",
      "DP : 4/16  epoch : 0  max_valid_acc = 0.7249000036716461\n",
      "DP : 4/16  epoch : 0 -- training cross-entropy : 0.1709511996358633\n",
      "DP : 4/16  epoch : 0 training_acc = 0.7264800015687942 valid_acc = 0.7249000036716461\n",
      "DP : 4/16  epoch : 1  max_valid_acc = 0.8070999974012375\n",
      "DP : 4/16  epoch : 2  max_valid_acc = 0.850699998140335\n",
      "DP : 4/16  epoch : 3  max_valid_acc = 0.8538000005483627\n",
      "DP : 4/16  epoch : 4  max_valid_acc = 0.8634000015258789\n",
      "DP : 4/16  epoch : 5  max_valid_acc = 0.8676000010967254\n",
      "DP : 4/16  epoch : 6  max_valid_acc = 0.8681000024080276\n",
      "DP : 4/16  epoch : 7  max_valid_acc = 0.8753000032901764\n",
      "DP : 4/16  epoch : 8  max_valid_acc = 0.8840000003576278\n",
      "DP : 4/16  epoch : 10  max_valid_acc = 0.8865000009536743\n",
      "DP : 4/16  epoch : 11  max_valid_acc = 0.8999999982118606\n",
      "DP : 4/16  epoch : 13  max_valid_acc = 0.9036999970674515\n",
      "DP : 4/16  epoch : 15  max_valid_acc = 0.9079000014066696\n",
      "DP : 4/16  epoch : 17  max_valid_acc = 0.9121000045537948\n",
      "DP : 4/16  epoch : 19  max_valid_acc = 0.9158000022172927\n",
      "DP : 4/16  epoch : 25  max_valid_acc = 0.9170000034570694\n",
      "DP : 4/16  epoch : 26  max_valid_acc = 0.9182000041007996\n",
      "DP : 4/16  epoch : 27  max_valid_acc = 0.919600003361702\n",
      "DP : 4/16  epoch : 28  max_valid_acc = 0.9207000035047531\n",
      "DP : 4/16  epoch : 30  max_valid_acc = 0.9229000014066696\n",
      "DP : 4/16  epoch : 33  max_valid_acc = 0.9238000029325485\n",
      "DP : 4/16  epoch : 40  max_valid_acc = 0.9243000048398972\n",
      "DP : 4/16  epoch : 42  max_valid_acc = 0.925600004196167\n",
      "DP : 4/16  epoch : 50 -- training cross-entropy : 0.02068767004646361\n",
      "DP : 4/16  epoch : 50 training_acc = 0.9394000022411346 valid_acc = 0.9235000026226043\n",
      "DP : 4/16  epoch : 55  max_valid_acc = 0.9288000017404556\n",
      "DP : 4/16  epoch : 71  max_valid_acc = 0.9303000003099442\n",
      "DP : 4/16  epoch : 73  max_valid_acc = 0.9311000019311905\n",
      "DP : 4/16  epoch : 100 -- training cross-entropy : 0.01529986737202853\n",
      "DP : 4/16  epoch : 100 training_acc = 0.9576800044775009 valid_acc = 0.9294000029563904\n",
      "DP : 4/16  epoch : 109  max_valid_acc = 0.9312000024318695\n",
      "DP : 4/16  epoch : 113  max_valid_acc = 0.9330000025033951\n",
      "DP : 4/16  epoch : 138  max_valid_acc = 0.9336000013351441\n",
      "DP : 4/16  epoch : 150 -- training cross-entropy : 0.011034383416175842\n",
      "DP : 4/16  epoch : 150 training_acc = 0.9680000064373017 valid_acc = 0.9284000027179719\n",
      "DP : 4/16  epoch : 200 -- training cross-entropy : 0.00965676139574498\n",
      "DP : 4/16  epoch : 200 training_acc = 0.9771600106954574 valid_acc = 0.9310000026226044\n",
      "DP : 4/16  epoch : 250 -- training cross-entropy : 0.009253540250938386\n",
      "DP : 4/16  epoch : 250 training_acc = 0.9784000115394592 valid_acc = 0.931200003027916\n",
      "DP : 4/16  epoch : 300 -- training cross-entropy : 0.009516282968921586\n",
      "DP : 4/16  epoch : 300 training_acc = 0.9782400114536285 valid_acc = 0.9311000031232833\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc_vec = {}\n",
    "for k in range(0,5):\n",
    "    drop_probability = k/16\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        epochs = 301\n",
    "        batch_size = 100\n",
    "        \n",
    "        tmp_xent_loss_3 = [1.0,1.0,1.0]\n",
    "        learning_rate = 1/2**10\n",
    "        rep_num = int((x_train.shape[0])/batch_size)\n",
    "        max_valid_acc = .0\n",
    "        valid_rep_num = int((x_valid.shape[0])/batch_size)\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            tmp_loss_vec = [.0 for a in range(rep_num)]\n",
    "            tmp_valid_acc_vec = [.0 for a in range(valid_rep_num)]\n",
    "            tmp_train_acc_vec = [.0 for a in range(rep_num)]\n",
    "            for j in range(rep_num):\n",
    "                batch_train_x, batch_train_y = train_minibatch_data.minibatch(batch_size)\n",
    "                feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : drop_probability, training : True, lr : learning_rate}\n",
    "                _, tmp_loss_vec[j] = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "            \n",
    "            tmp_xent_loss_3 = [tmp_xent_loss_3[1], tmp_xent_loss_3[2], sum(tmp_loss_vec)/rep_num]\n",
    "            \n",
    "            if tmp_xent_loss_3[0] == min(tmp_xent_loss_3):\n",
    "                learning_rate = learning_rate * 7/8\n",
    "            \n",
    "            for j in range(valid_rep_num):\n",
    "                batch_valid_x, batch_valid_y = valid_minibatch_data.minibatch(batch_size)\n",
    "                feed_dict = {x : batch_valid_x, y : batch_valid_y, drop_prob : drop_probability, training : False}\n",
    "                tmp_valid_acc_vec[j] = sess.run(acc, feed_dict = feed_dict)\n",
    "\n",
    "            valid_acc = sum(tmp_valid_acc_vec)/valid_rep_num\n",
    "    \n",
    "            if valid_acc > max_valid_acc:\n",
    "                max_valid_acc = valid_acc\n",
    "                best_valid_acc_vec[k] = max_valid_acc\n",
    "                print(\"DP : \" + str(k) + \"/16  epoch : \" + str(i) + \"  max_valid_acc = \" + str(valid_acc))\n",
    "                save_path = saver.save(sess, \"./CNNres/model\" + str(k) + \".ckpt\")\n",
    "        \n",
    "            if i % 50 == 0:\n",
    "                print(\"DP : \" + str(k) + \"/16  epoch : \" + str(i) + \" -- training cross-entropy : \" + str(tmp_xent_loss_3[2]))\n",
    "                for j in range(rep_num):\n",
    "                    batch_train_x, batch_train_y = train_minibatch_data.minibatch(batch_size)\n",
    "                    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : drop_probability, training : False}\n",
    "                    tmp_train_acc_vec[j] = sess.run(acc, feed_dict = feed_dict)\n",
    "                    \n",
    "                train_acc = sum(tmp_train_acc_vec)/rep_num\n",
    "                print(\"DP : \" + str(k) + \"/16  epoch : \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid_acc))\n",
    "\n",
    "            if (tmp_xent_loss_3[0] - tmp_xent_loss_3[1])**2 + (tmp_xent_loss_3[1] - tmp_xent_loss_3[2])**2 < 1e-10:\n",
    "                print(\"DP : \" + str(k) + \"/16  converged\" + \"  epoch : \" + str(i))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.9188999998569488, 1: 0.9296000015735626, 2: 0.9336000019311905, 3: 0.9338000029325485, 4: 0.9336000013351441}\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "print(best_valid_acc_vec)\n",
    "print(max(best_valid_acc_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./CNNres/model3.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver.restore(sess, \"./CNNres/model3.ckpt\")\n",
    "print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "rep_num = int((x_train.shape[0])/batch_size)\n",
    "tmp_train_acc_vec = [.0 for a in range(rep_num)]\n",
    "CNNres_predict_train = []\n",
    "\n",
    "for j in range(rep_num):\n",
    "    batch_train_x, batch_train_y = train_minibatch_data.minibatch(batch_size)\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : 1/8, training : False}\n",
    "    tmp_CNNres_predict_train, tmp_train_acc_vec[j] = sess.run([pred_labels,acc], feed_dict = feed_dict)\n",
    "    CNNres_predict_train = np.concatenate([CNNres_predict_train, tmp_CNNres_predict_train])\n",
    "\n",
    "CNNres_train_acc = sum(tmp_train_acc_vec)/rep_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4924    0    1    0    0    0   28    0    0    0]\n",
      " [   0 5016    0    0    0    0    0    0    0    0]\n",
      " [  11    0 4958    1    4    0   54    0    0    0]\n",
      " [   6    0    2 4919    0    0    7    0    0    0]\n",
      " [   1    0   36   20 5002    0   48    0    0    0]\n",
      " [   0    0    0    0    0 4992    0    0    0    0]\n",
      " [  67    0    4    1    5    0 4855    0    0    0]\n",
      " [   0    0    0    0    0    0    0 5050    0   32]\n",
      " [   0    0    0    0    0    0    0    0 4980    0]\n",
      " [   0    0    0    0    0    0    0    1    0 4975]]\n",
      "TRAINING ACCURACY = 0.9934199929237366\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(CNNres_predict_train,y_train))\n",
    "print(\"TRAINING ACCURACY =\",CNNres_train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "valid_rep_num = int((x_valid.shape[0])/batch_size)\n",
    "tmp_valid_acc_vec = [.0 for a in range(rep_num)]\n",
    "CNNres_predict_valid = []\n",
    "\n",
    "for j in range(valid_rep_num):\n",
    "    batch_valid_x, batch_valid_y = valid_minibatch_data.minibatch(batch_size)\n",
    "    feed_dict = {x : batch_valid_x, y : batch_valid_y, drop_prob : 1/8, training : False}\n",
    "    tmp_CNNres_predict_valid, tmp_valid_acc_vec[j] = sess.run([pred_labels,acc], feed_dict = feed_dict)\n",
    "    CNNres_predict_valid = np.concatenate([CNNres_predict_valid, tmp_CNNres_predict_valid])\n",
    "\n",
    "CNNres_valid_acc = sum(tmp_valid_acc_vec)/valid_rep_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 873    1    9   12    0    0   91    0    1    0]\n",
      " [   2  978    0    3    1    0    1    0    1    0]\n",
      " [  18    0  911   11   16    0   63    0    3    0]\n",
      " [  15    4    3  990   16    0   18    0    1    0]\n",
      " [   1    0   41   29  929    0   67    0    4    0]\n",
      " [   0    0    1    0    0  986    0    1    3    2]\n",
      " [  78    1   33   14   25    0  765    0    2    0]\n",
      " [   0    0    0    0    0   18    0  937    2   25]\n",
      " [   4    0    1    0    2    2    3    0 1003    0]\n",
      " [   0    0    0    0    0    2    0   11    0  966]]\n",
      "VALIDATION ACCURACY = 0.9337999999523163\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(CNNres_predict_valid,y_valid))\n",
    "print(\"VALIDATION ACCURACY =\",CNNres_valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN_ACC': 0.9934199929237366, 'VALID_ACC': 0.9337999999523163}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"TRAIN_ACC\" : CNNres_train_acc , \"VALID_ACC\" : CNNres_valid_acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "test_rep_num = int((x_test.shape[0])/batch_size)\n",
    "tmp_test_acc_vec = [.0 for a in range(rep_num)]\n",
    "CNNres_predict_test = []\n",
    "\n",
    "for j in range(test_rep_num):\n",
    "    batch_test_x, batch_test_y = test_minibatch_data.minibatch(batch_size)\n",
    "    feed_dict = {x : batch_test_x, y : batch_test_y, drop_prob : 1/8, training : False}\n",
    "    tmp_CNNres_predict_test, tmp_test_acc_vec[j] = sess.run([pred_labels,acc], feed_dict = feed_dict)\n",
    "    CNNres_predict_test = np.concatenate([CNNres_predict_test, tmp_CNNres_predict_test])\n",
    "\n",
    "CNNres_test_acc = sum(tmp_test_acc_vec)/test_rep_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[874   1  13  15   0   0  77   0   1   0]\n",
      " [  0 996   0   4   0   0   1   0   0   1]\n",
      " [ 29   0 913   5  19   1  48   0   0   0]\n",
      " [ 10   1   7 928  18   1  24   0   1   0]\n",
      " [  2   2  40  33 923   0  55   0   3   0]\n",
      " [  0   0   0   0   0 976   0   1   1   0]\n",
      " [ 83   0  27  13  40   0 793   0   3   0]\n",
      " [  0   0   0   0   0  15   0 984   2  32]\n",
      " [  2   0   0   1   0   1   2   0 988   1]\n",
      " [  0   0   0   1   0   6   0  15   1 966]]\n",
      "TEST ACCURACY = 0.9340999960899353\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(CNNres_predict_test,y_test))\n",
    "print(\"TEST ACCURACY =\",CNNres_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN_ACC': 0.9934199929237366,\n",
       " 'VALID_ACC': 0.9337999999523163,\n",
       " 'TEST_ACC': 0.9340999960899353}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"TRAIN_ACC\" : CNNres_train_acc , \"VALID_ACC\" : CNNres_valid_acc , \"TEST_ACC\" : CNNres_test_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
