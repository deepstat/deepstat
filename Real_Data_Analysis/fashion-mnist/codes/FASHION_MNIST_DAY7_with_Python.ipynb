{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASHION MNIST with Python (DAY 7)\n",
    "\n",
    "DATA SOURCE : https://www.kaggle.com/zalando-research/fashionmnist (Kaggle, Fashion MNIST)\n",
    "\n",
    "FASHION MNIST with Python (DAY 1) : http://deepstat.tistory.com/35\n",
    "\n",
    "FASHION MNIST with Python (DAY 2) : http://deepstat.tistory.com/36\n",
    "\n",
    "FASHION MNIST with Python (DAY 3) : http://deepstat.tistory.com/37\n",
    "\n",
    "FASHION MNIST with Python (DAY 4) : http://deepstat.tistory.com/38\n",
    "\n",
    "FASHION MNIST with Python (DAY 5) : http://deepstat.tistory.com/39\n",
    "\n",
    "FASHION MNIST with Python (DAY 6) : http://deepstat.tistory.com/40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing numpy, pandas, pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"..\\\\datasets\\\\fashion-mnist_train.csv\")\n",
    "data_test = pd.read_csv(\"..\\\\datasets\\\\fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train_y = data_train.label\n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x = data_train.drop(\"label\",axis=1)/256\n",
    "x_test = data_test.drop(\"label\",axis=1)/256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting valid and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "valid2_idx = np.random.choice(60000,10000,replace = False)\n",
    "valid1_idx = np.random.choice(list(set(range(60000)) - set(valid2_idx)),10000,replace=False)\n",
    "train_idx = list(set(range(60000))-set(valid1_idx)-set(valid2_idx))\n",
    "\n",
    "x_train = data_train_x.iloc[train_idx,:]\n",
    "y_train = data_train_y.iloc[train_idx]\n",
    "\n",
    "x_valid1 = data_train_x.iloc[valid1_idx,:]\n",
    "y_valid1 = data_train_y.iloc[valid1_idx]\n",
    "\n",
    "x_valid2 = data_train_x.iloc[valid2_idx,:]\n",
    "y_valid2 = data_train_y.iloc[valid2_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining weight_variables and bias_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variables(shape):\n",
    "    initial = tf.truncated_normal(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variables(shape):\n",
    "    initial = tf.truncated_normal(shape)\n",
    "    return tf.Variable(initial)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the MLP\n",
    "\n",
    "leaky ReLU, Dropout, Maxout, Batch Normalization, softmax, cross entropy, Adam\n",
    "\n",
    "- Model : input -> [inner product -> leaky_relu -> dropout] -> [batch normalization -> inner product -> reshape -> maxout -> dropout] -> [inner product -> leaky_relu -> dropout] -> [batch Normalization -> inner product -> reshape -> maxout -> softmax] -> output\n",
    "\n",
    "- Loss : cross entropy\n",
    "\n",
    "- Optimizer : Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None,784])\n",
    "y = tf.placeholder(\"int64\", [None,])\n",
    "y_dummies = tf.one_hot(y,depth = 10)\n",
    "\n",
    "drop_prob = tf.placeholder(\"float\")\n",
    "training = tf.placeholder(\"bool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer1\n",
    "\n",
    "[inner product -> leaky_relu -> dropout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_w = weight_variables([784,640])\n",
    "l1_b = bias_variables([640])\n",
    "l1_inner_product = tf.matmul(x, l1_w) + l1_b\n",
    "l1_leaky_relu = tf.nn.leaky_relu(l1_inner_product)\n",
    "l1_dropout = tf.layers.dropout(l1_leaky_relu,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer2\n",
    "\n",
    "[batch normalization -> inner product -> reshape -> maxout -> dropout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_w = weight_variables([640,640])\n",
    "l2_b = bias_variables([640])\n",
    "l2_batch_normalization = tf.layers.batch_normalization(l1_dropout, training = training)\n",
    "l2_inner_product = tf.matmul(l2_batch_normalization, l2_w) + l2_b\n",
    "l2_reshape = tf.reshape(l2_inner_product,[-1,80,8])\n",
    "l2_maxout = tf.reshape(\n",
    "    tf.contrib.layers.maxout(l2_reshape,num_units=1),\n",
    "    [-1,80])\n",
    "l2_dropout = tf.layers.dropout(l2_maxout,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer3\n",
    "\n",
    "[inner product -> leaky_relu -> dropout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_w = weight_variables([80,80])\n",
    "l3_b = bias_variables([80])\n",
    "l3_inner_product = tf.matmul(l2_dropout, l3_w) + l3_b\n",
    "l3_leaky_relu = tf.nn.leaky_relu(l3_inner_product)\n",
    "l3_dropout = tf.layers.dropout(l3_leaky_relu,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer4\n",
    "\n",
    "[batch normalization -> inner product -> reshape -> maxout -> softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_w = weight_variables([80,80])\n",
    "l4_b = bias_variables([80])\n",
    "l4_batch_normalization = tf.layers.batch_normalization(l3_dropout, training = training)\n",
    "l4_inner_product = tf.matmul(l4_batch_normalization, l4_w) + l4_b\n",
    "l4_reshape = tf.reshape(l4_inner_product,[-1,10,8])\n",
    "l4_maxout = tf.reshape(\n",
    "    tf.contrib.layers.maxout(l4_reshape,num_units=1),\n",
    "    [-1,10])\n",
    "l4_log_softmax = tf.nn.log_softmax(l4_maxout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_loss = -tf.reduce_sum( tf.multiply(y_dummies,l4_log_softmax) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = tf.argmax(l4_log_softmax,axis=1)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(y, pred_labels),\"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.placeholder(\"float\")\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(xent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 627.51556\n",
      "step 0 training_acc = 0.102275 valid_acc = 0.1025\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 10000 training cross-entropy : 16.393414\n",
      "step 20000 training cross-entropy : 3.653322\n",
      "step 30000 training cross-entropy : 1.637823\n",
      "step 40000 training cross-entropy : 11.408547\n",
      "step 40000 training_acc = 0.9137 valid_acc = 0.8486\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 50000 training cross-entropy : 5.169942\n",
      "step 60000 training cross-entropy : 1.0102936\n",
      "step 70000 training cross-entropy : 7.9847817\n",
      "step 80000 training cross-entropy : 1.5986788\n",
      "step 80000 training_acc = 0.8801 valid_acc = 0.8151\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 90000 training cross-entropy : 6.7976327\n",
      "step 100000 training cross-entropy : 0.26217932\n",
      "step 110000 training cross-entropy : 1.1882036\n",
      "step 120000 training cross-entropy : 0.4843976\n",
      "step 120000 training_acc = 0.955025 valid_acc = 0.8604\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 130000 training cross-entropy : 2.9634778\n",
      "step 140000 training cross-entropy : 0.64811546\n",
      "step 150000 training cross-entropy : 0.048579566\n",
      "step 160000 training cross-entropy : 0.21984404\n",
      "step 160000 training_acc = 0.96965 valid_acc = 0.8702\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 170000 training cross-entropy : 1.0292068\n",
      "step 180000 training cross-entropy : 0.08035797\n",
      "step 190000 training cross-entropy : 0.5936426\n",
      "step 200000 training cross-entropy : 1.7777281\n",
      "step 200000 training_acc = 0.961925 valid_acc = 0.8643\n",
      "Model saved in path: ./MLP/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "for i in range(200001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .15, training : True, lr : 0.01}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 40000 == 0:\n",
    "        feed_dict = {x : x_train, y : y_train, drop_prob : .15, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .15, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./MLP/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 0.19749565\n",
      "step 0 training_acc = 0.961825 valid_acc = 0.8644\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 10000 training cross-entropy : 0.0027275973\n",
      "step 20000 training cross-entropy : 0.010455683\n",
      "step 30000 training cross-entropy : 0.015472678\n",
      "step 40000 training cross-entropy : 0.21760906\n",
      "step 40000 training_acc = 0.981975 valid_acc = 0.8739\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 50000 training cross-entropy : 8.4638505e-06\n",
      "step 60000 training cross-entropy : 0.0029786504\n",
      "step 70000 training cross-entropy : 0.050034236\n",
      "step 80000 training cross-entropy : 0.0038360865\n",
      "step 80000 training_acc = 0.979325 valid_acc = 0.8714\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 90000 training cross-entropy : 0.012144463\n",
      "step 100000 training cross-entropy : 0.00048363706\n",
      "step 110000 training cross-entropy : 0.00011801363\n",
      "step 120000 training cross-entropy : 0.0042042355\n",
      "step 120000 training_acc = 0.982 valid_acc = 0.875\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 130000 training cross-entropy : 2.1696038e-05\n",
      "step 140000 training cross-entropy : 0.32488036\n",
      "step 150000 training cross-entropy : 3.3378572e-06\n",
      "step 160000 training cross-entropy : 0.006305409\n",
      "step 160000 training_acc = 0.984975 valid_acc = 0.8764\n",
      "Model saved in path: ./MLP/model.ckpt\n",
      "step 170000 training cross-entropy : 0.00022015534\n",
      "step 180000 training cross-entropy : 0.028006688\n",
      "step 190000 training cross-entropy : 8.439751e-05\n",
      "step 200000 training cross-entropy : 2.217285e-05\n",
      "step 200000 training_acc = 0.989475 valid_acc = 0.8823\n",
      "Model saved in path: ./MLP/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "for i in range(200001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .15, training : True, lr : 0.001}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 10000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 40000 == 0:\n",
    "        feed_dict = {x : x_train, y : y_train, drop_prob : .15, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .15, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./MLP/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {x : x_train, y : y_train, drop_prob : .15, training : False}\n",
    "MLP_predict_train, MLP_train_acc = sess.run([pred_labels,acc], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3989    0    0    1    2    0  145    1    0    0]\n",
      " [   0 3990    0   18    1    0    1   20    0    0]\n",
      " [   4    0 3935    5    4    0    5    1    0    0]\n",
      " [   0    0    0 3895    0    0    1    0    0    0]\n",
      " [   0    0  119    7 4009    0   61    0    0    0]\n",
      " [   0    0    0    0    0 3932    0    9    0    0]\n",
      " [   0    0    0    0    0    0 3789    0    0    0]\n",
      " [   0    0    0    0    0    0    0 4065    0    0]\n",
      " [   1    0    2    3    0    0    3    7 3946    0]\n",
      " [   0    0    0    0    0    0    0    0    0 4029]]\n",
      "TRAINING ACCURACY = 0.989475\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(MLP_predict_train,y_train))\n",
    "print(\"TRAINING ACCURACY =\",MLP_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .15, training : False}\n",
    "MLP_predict_valid1, MLP_valid1_acc = sess.run([pred_labels,acc], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 921    3   16   56   12    0  191    1    7    0]\n",
      " [   4 1012    1   26    5    0    2    7    0    0]\n",
      " [  18    1  734   10   61    0   74    0    3    0]\n",
      " [  17    5   10  867   19    0   15    0    3    0]\n",
      " [   2    0  142   32  858    0  122    0    3    0]\n",
      " [   0    0    0    0    2 1043    0   34    2   11]\n",
      " [  45    2   33   10   26    0  558    0    1    0]\n",
      " [   0    0    0    0    0    5    0  869    0   17]\n",
      " [   8    2    9   10   12    5   25    6 1015    4]\n",
      " [   0    1    0    1    0    7    0   31    0  946]]\n",
      "VALIDATION ACCURACY = 0.8823\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(MLP_predict_valid1,y_valid1))\n",
    "print(\"VALIDATION ACCURACY =\",MLP_valid1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN_ACC': 0.989475, 'VALID_ACC': 0.8823}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"TRAIN_ACC\" : MLP_train_acc , \"VALID_ACC\" : MLP_valid1_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
