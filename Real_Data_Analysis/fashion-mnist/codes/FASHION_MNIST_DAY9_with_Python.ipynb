{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASHION MNIST with Python (DAY 9)\n",
    "\n",
    "DATA SOURCE : https://www.kaggle.com/zalando-research/fashionmnist (Kaggle, Fashion MNIST)\n",
    "\n",
    "FASHION MNIST with Python (DAY 1) : http://deepstat.tistory.com/35\n",
    "\n",
    "FASHION MNIST with Python (DAY 2) : http://deepstat.tistory.com/36\n",
    "\n",
    "FASHION MNIST with Python (DAY 3) : http://deepstat.tistory.com/37\n",
    "\n",
    "FASHION MNIST with Python (DAY 4) : http://deepstat.tistory.com/38\n",
    "\n",
    "FASHION MNIST with Python (DAY 5) : http://deepstat.tistory.com/39\n",
    "\n",
    "FASHION MNIST with Python (DAY 6) : http://deepstat.tistory.com/40\n",
    "\n",
    "FASHION MNIST with Python (DAY 7) : http://deepstat.tistory.com/41\n",
    "\n",
    "FASHION MNIST with Python (DAY 8) : http://deepstat.tistory.com/42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing numpy, pandas, pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../datasets/fashion-mnist_train.csv\")\n",
    "data_test = pd.read_csv(\"../datasets/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train_y = data_train.label\n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x = data_train.drop(\"label\",axis=1)/256\n",
    "x_test = data_test.drop(\"label\",axis=1)/256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting valid and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "valid2_idx = np.random.choice(60000,10000,replace = False)\n",
    "valid1_idx = np.random.choice(list(set(range(60000)) - set(valid2_idx)),10000,replace=False)\n",
    "train_idx = list(set(range(60000))-set(valid1_idx)-set(valid2_idx))\n",
    "\n",
    "x_train = data_train_x.iloc[train_idx,:]\n",
    "y_train = data_train_y.iloc[train_idx]\n",
    "\n",
    "x_valid1 = data_train_x.iloc[valid1_idx,:]\n",
    "y_valid1 = data_train_y.iloc[valid1_idx]\n",
    "\n",
    "x_valid2 = data_train_x.iloc[valid2_idx,:]\n",
    "y_valid2 = data_train_y.iloc[valid2_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP with re-using variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining weight_variables and bias_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variables(shape):\n",
    "    initial = tf.truncated_normal(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variables(shape):\n",
    "    initial = tf.truncated_normal(shape)\n",
    "    return tf.Variable(initial)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the MLP with re-using variables\n",
    "\n",
    "Linear, ReLU, leaky ReLU, ELU, SELU, Sigmoid, arctan, tanh, softsign, softplus, softmax, Maxout, Dropout, Batch Normalization, cross entropy, Adam\n",
    "\n",
    "- Model : input -> [inner product -> dropout]-> [batch normalization -> inner product -> [Linear, ReLU, leaky ReLU, ELU, SELU, Sigmoid, arctan, tanh, softsign, softplus, softmax, Maxout]\\*20 -> dropout]\\*10 -> [batch normalization -> inner product -> softmax] -> output\n",
    "\n",
    "- Loss : cross entropy\n",
    "\n",
    "- Optimizer : Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_reuse_layer(inputs, training, drop_prob):\n",
    "    with tf.variable_scope(\"deepstat\", reuse=tf.AUTO_REUSE):\n",
    "        w_linear = tf.get_variable(\"w_linear\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_linear = tf.get_variable(\"b_linear\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_relu = tf.get_variable(\"w_relu\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_relu = tf.get_variable(\"b_relu\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_leaky_relu = tf.get_variable(\"w_leaky_relu\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_leaky_relu = tf.get_variable(\"b_leaky_relu\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_elu = tf.get_variable(\"w_elu\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_elu = tf.get_variable(\"b_elu\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_selu = tf.get_variable(\"w_selu\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_selu = tf.get_variable(\"b_selu\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_sigmoid = tf.get_variable(\"w_sigmoid\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_sigmoid = tf.get_variable(\"b_sigmoid\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_atan = tf.get_variable(\"w_atan\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_atan = tf.get_variable(\"b_atan\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_tanh = tf.get_variable(\"w_tanh\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_tanh = tf.get_variable(\"b_tanh\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_softsign = tf.get_variable(\"w_softsign\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_softsign = tf.get_variable(\"b_softsign\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_softplus = tf.get_variable(\"w_softplus\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_softplus = tf.get_variable(\"b_softplus\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_log_softmax = tf.get_variable(\"w_log_softmax\", [1920,160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_log_softmax = tf.get_variable(\"b_log_softmax\", [160], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        w_maxout = tf.get_variable(\"w_maxout\", [1920,320], initializer = tf.initializers.random_uniform(-1,1))\n",
    "        b_maxout = tf.get_variable(\"b_maxout\", [320], initializer = tf.initializers.random_uniform(-1,1))\n",
    "    \n",
    "    l_batch_normalization = tf.layers.batch_normalization(inputs, training = training)\n",
    "    l_linear = tf.matmul(l_batch_normalization, w_linear) + b_linear\n",
    "    l_relu = tf.nn.relu(tf.matmul(l_batch_normalization, w_relu) + b_relu)\n",
    "    l_leaky_relu = tf.nn.leaky_relu(tf.matmul(l_batch_normalization, w_leaky_relu) + b_leaky_relu)\n",
    "    l_elu = tf.nn.elu(tf.matmul(l_batch_normalization, w_elu) + b_elu)\n",
    "    l_selu = tf.nn.selu(tf.matmul(l_batch_normalization, w_selu) + b_selu)\n",
    "    l_sigmoid = tf.nn.sigmoid(tf.matmul(l_batch_normalization, w_sigmoid) + b_sigmoid)\n",
    "    l_atan = tf.atan(tf.matmul(l_batch_normalization, w_atan) + b_atan)\n",
    "    l_tanh = tf.nn.tanh(tf.matmul(l_batch_normalization, w_tanh) + b_tanh)\n",
    "    l_softsign = tf.nn.softsign(tf.matmul(l_batch_normalization, w_softsign) + b_softsign)\n",
    "    l_softplus = tf.nn.softplus(tf.matmul(l_batch_normalization, w_softplus) + b_softplus)\n",
    "    l_log_softmax = tf.nn.log_softmax(tf.matmul(l_batch_normalization, w_log_softmax) + b_log_softmax)\n",
    "    l_maxout = tf.reshape(\n",
    "        tf.contrib.layers.maxout(\n",
    "            tf.reshape(\n",
    "                tf.matmul(\n",
    "                    l_batch_normalization, w_maxout) + b_maxout,\n",
    "                [-1,160,2]),\n",
    "            num_units=1),\n",
    "        [-1,160])\n",
    "    \n",
    "    l_concat = tf.concat([\n",
    "        l_linear,l_relu,l_leaky_relu,l_elu,l_selu,l_sigmoid,\n",
    "        l_atan,l_tanh,l_softsign,l_softplus,l_log_softmax,l_maxout\n",
    "        ], 1)\n",
    "    l_dropout = tf.layers.dropout(l_concat, rate = drop_prob, training = training)\n",
    "    return l_dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None,784])\n",
    "y = tf.placeholder(\"int64\", [None,])\n",
    "y_dummies = tf.one_hot(y,depth = 10)\n",
    "\n",
    "drop_prob = tf.placeholder(\"float\")\n",
    "training = tf.placeholder(\"bool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer1\n",
    "\n",
    "[inner product -> dropout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_w = weight_variables([784,1920])\n",
    "l1_b = bias_variables([1920])\n",
    "l1_inner_product = tf.matmul(x, l1_w) + l1_b\n",
    "l1_dropout = tf.layers.dropout(l1_inner_product,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer2-11\n",
    "\n",
    "[batch normalization -> inner product -> [Linear, ReLU, leaky ReLU, ELU, SELU, Sigmoid, arctan, tanh, softsign, softplus, softmax, Maxout]\\*20 -> dropout]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2 = weight_reuse_layer(l1_dropout, training, drop_prob)\n",
    "l3 = weight_reuse_layer(l2, training, drop_prob)\n",
    "l4 = weight_reuse_layer(l3, training, drop_prob)\n",
    "l5 = weight_reuse_layer(l4, training, drop_prob)\n",
    "l6 = weight_reuse_layer(l5, training, drop_prob)\n",
    "l7 = weight_reuse_layer(l6, training, drop_prob)\n",
    "l8 = weight_reuse_layer(l7, training, drop_prob)\n",
    "l9 = weight_reuse_layer(l8, training, drop_prob)\n",
    "l10 = weight_reuse_layer(l9, training, drop_prob)\n",
    "l11 = weight_reuse_layer(l10, training, drop_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer12\n",
    "\n",
    "[batch normalization -> inner product -> softmax]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l12_w = weight_variables([1920,10])\n",
    "l12_b = bias_variables([10])\n",
    "l12_batch_normalization =  tf.layers.batch_normalization(l11, training = training)\n",
    "l12_inner_product = tf.matmul(l12_batch_normalization, l12_w) + l12_b\n",
    "l12_log_softmax = tf.nn.log_softmax(l12_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_loss = -tf.reduce_sum( tf.multiply(y_dummies,l12_log_softmax) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = tf.argmax(l12_log_softmax,axis=1)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(y, pred_labels),\"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.placeholder(\"float\")\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(xent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 31872.088\n",
      "step 0 training_acc = 0.1004 valid_acc = 0.0995\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 2000 training cross-entropy : 583.2489\n",
      "step 4000 training cross-entropy : 1131.0829\n",
      "step 4000 training_acc = 0.0983 valid_acc = 0.106\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 6000 training cross-entropy : 1343.8828\n",
      "step 8000 training cross-entropy : 970.83813\n",
      "step 8000 training_acc = 0.0992 valid_acc = 0.1006\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 10000 training cross-entropy : 1214.2537\n",
      "step 12000 training cross-entropy : 825.8395\n",
      "step 12000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 14000 training cross-entropy : 800.1408\n",
      "step 16000 training cross-entropy : 833.8031\n",
      "step 16000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 18000 training cross-entropy : 932.93713\n",
      "step 20000 training cross-entropy : 690.29736\n",
      "step 20000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "for i in range(20001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : True, lr : 0.1}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 2000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 4000 == 0:\n",
    "        feed_dict = {x : x_train, y : y_train, drop_prob : .125, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./MLP_reuse/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 664.647\n",
      "step 0 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 8000 training cross-entropy : 392.25644\n",
      "step 16000 training cross-entropy : 345.9796\n",
      "step 16000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 24000 training cross-entropy : 343.65155\n",
      "step 32000 training cross-entropy : 281.71674\n",
      "step 32000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 40000 training cross-entropy : 210.97086\n",
      "step 48000 training cross-entropy : 263.9254\n",
      "step 48000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 56000 training cross-entropy : 255.4663\n",
      "step 64000 training cross-entropy : 202.80786\n",
      "step 64000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 72000 training cross-entropy : 187.29158\n",
      "step 80000 training cross-entropy : 170.4274\n",
      "step 80000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "for i in range(80001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : True, lr : 0.01}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 8000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 16000 == 0:\n",
    "        feed_dict = {x : x_train, y : y_train, drop_prob : .125, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./MLP_reuse/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 211.09633\n",
      "step 0 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 32000 training cross-entropy : 147.32101\n",
      "step 64000 training cross-entropy : 169.66533\n",
      "step 64000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 96000 training cross-entropy : 218.44417\n",
      "step 128000 training cross-entropy : 139.58224\n",
      "step 128000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 160000 training cross-entropy : 135.28773\n",
      "step 192000 training cross-entropy : 125.1377\n",
      "step 192000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 224000 training cross-entropy : 121.90304\n",
      "step 256000 training cross-entropy : 116.033875\n",
      "step 256000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 288000 training cross-entropy : 137.7208\n",
      "step 320000 training cross-entropy : 125.229385\n",
      "step 320000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "for i in range(320001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : True, lr : 0.001}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 32000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 64000 == 0:\n",
    "        feed_dict = {x : x_train, y : y_train, drop_prob : .125, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./MLP_reuse/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 106.5896\n",
      "step 0 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 128000 training cross-entropy : 115.030716\n",
      "step 256000 training cross-entropy : 152.09715\n",
      "step 256000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 384000 training cross-entropy : 155.68195\n",
      "step 512000 training cross-entropy : 118.50292\n",
      "step 512000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n",
      "step 640000 training cross-entropy : 123.75328\n",
      "step 768000 training cross-entropy : 107.267\n",
      "step 768000 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f9c7dcdd5355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbatch_train_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_obs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_train_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mbatch_train_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_prob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m.125\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;36m0.0001\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxent_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m128000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "for i in range(1280001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : True, lr : 0.0001}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 128000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 256000 == 0:\n",
    "        feed_dict = {x : x_train, y : y_train, drop_prob : .125, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./MLP_reuse/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 818348 training cross-entropy : 115.68921 accuracy of training step : 0.91796875\n",
      "step 818348 training_acc = 0.09985 valid_acc = 0.1015\n",
      "Model saved in path: ./MLP_reuse/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "batch_train_x = x_train.iloc[batch_obs]\n",
    "batch_train_y = y_train.iloc[batch_obs]\n",
    "feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : True, lr : 0.0001}\n",
    "_, tmp,tmp_acc = sess.run([train_step,xent_loss,acc], feed_dict = feed_dict)\n",
    "\n",
    "print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp) + \" accuracy of training step : \" + str(tmp_acc))\n",
    "feed_dict = {x : x_train, y : y_train, drop_prob : .125, training : False}\n",
    "train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "save_path = saver.save(sess, \"./MLP_reuse/model.ckpt\")\n",
    "print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {x : x_train, y : y_train, drop_prob : .125, training : True}\n",
    "MLP_predict_train, MLP_train_acc = sess.run([pred_labels,acc], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3559    2   48   32    2    0  600    0    1    0]\n",
      " [   5 3911    3    9    2    0    3    0    2    0]\n",
      " [  15    0 3138    2  107    0  129    0    1    0]\n",
      " [ 135   64   57 3781  195    1  114    0   15    0]\n",
      " [   7    2  496   58 3539    0  232    0    5    0]\n",
      " [   0    0    0    1    0 3901    0   58    7   21]\n",
      " [ 221    5  283   30  109    0 2853    0    6    0]\n",
      " [   0    0    0    0    0   21    0 3998    5  141]\n",
      " [  52    6   31   16   62    6   74    4 3900    9]\n",
      " [   0    0    0    0    0    3    0   43    4 3858]]\n",
      "TRAINING ACCURACY = 0.91095\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(MLP_predict_train,y_train))\n",
    "print(\"TRAINING ACCURACY =\",MLP_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : True}\n",
    "MLP_predict_valid1, MLP_valid1_acc = sess.run([pred_labels,acc], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 871    5   17   22    2    0  143    0    2    0]\n",
      " [   4  999    0    9    3    0    1    0    0    0]\n",
      " [   3    2  669    2   42    0   61    0    2    0]\n",
      " [  49   18   21  939   75    0   39    0   11    0]\n",
      " [   0    1  137   25  797    0   83    0    2    0]\n",
      " [   0    0    0    1    0 1022    0   27    6   16]\n",
      " [  80    1   89   10   59    0  637    0    6    0]\n",
      " [   0    0    0    0    0   21    0  896    1   37]\n",
      " [   8    0   12    4   17    8   23    5 1000   10]\n",
      " [   0    0    0    0    0    9    0   20    4  915]]\n",
      "VALIDATION ACCURACY = 0.8745\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(MLP_predict_valid1,y_valid1))\n",
    "print(\"VALIDATION ACCURACY =\",MLP_valid1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN_ACC': 0.91095, 'VALID_ACC': 0.8745}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"TRAIN_ACC\" : MLP_train_acc , \"VALID_ACC\" : MLP_valid1_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
