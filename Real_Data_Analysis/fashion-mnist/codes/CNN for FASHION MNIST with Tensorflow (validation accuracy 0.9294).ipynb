{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for FASHION MNIST with Tensorflow (validation accuracy 0.9294)\n",
    "\n",
    "DATA SOURCE : https://www.kaggle.com/zalando-research/fashionmnist (Kaggle, Fashion MNIST)\n",
    "\n",
    "* FASHION MNIST with Python (DAY 1) : http://deepstat.tistory.com/35\n",
    "* FASHION MNIST with Python (DAY 2) : http://deepstat.tistory.com/36\n",
    "* FASHION MNIST with Python (DAY 3) : http://deepstat.tistory.com/37\n",
    "* FASHION MNIST with Python (DAY 4) : http://deepstat.tistory.com/38\n",
    "* FASHION MNIST with Python (DAY 5) : http://deepstat.tistory.com/39\n",
    "* FASHION MNIST with Python (DAY 6) : http://deepstat.tistory.com/40\n",
    "* FASHION MNIST with Python (DAY 7) : http://deepstat.tistory.com/41\n",
    "* FASHION MNIST with Python (DAY 8) : http://deepstat.tistory.com/42\n",
    "* FASHION MNIST with Python (DAY 9) : http://deepstat.tistory.com/43\n",
    "* FASHION MNIST with Python (DAY 10) : http://deepstat.tistory.com/44"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing numpy, pandas, pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../datasets/fashion-mnist_train.csv\")\n",
    "data_test = pd.read_csv(\"../datasets/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train_y = data_train.label\n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x = data_train.drop(\"label\",axis=1)/256\n",
    "x_test = data_test.drop(\"label\",axis=1)/256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting valid and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "valid2_idx = np.random.choice(60000,10000,replace = False)\n",
    "valid1_idx = np.random.choice(list(set(range(60000)) - set(valid2_idx)),10000,replace=False)\n",
    "train_idx = list(set(range(60000))-set(valid1_idx)-set(valid2_idx))\n",
    "\n",
    "x_train = data_train_x.iloc[train_idx,:]\n",
    "y_train = data_train_y.iloc[train_idx]\n",
    "\n",
    "x_valid1 = data_train_x.iloc[valid1_idx,:]\n",
    "y_valid1 = data_train_y.iloc[valid1_idx]\n",
    "\n",
    "x_valid2 = data_train_x.iloc[valid2_idx,:]\n",
    "y_valid2 = data_train_y.iloc[valid2_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Class Minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class minibatchData:\n",
    "    def __init__(self, X, Y):\n",
    "        self.start_num = 0\n",
    "        self.x = X\n",
    "        self.y = Y\n",
    "\n",
    "    def minibatch(self, batch_size):\n",
    "        self.outidx = range(self.start_num,(self.start_num + batch_size))\n",
    "        self.start_num = (self.start_num + batch_size)%(self.x.shape[0])\n",
    "        return self.x.iloc[self.outidx,:], self.y.iloc[self.outidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_minibatch_data = minibatchData(x_train, y_train)\n",
    "valid1_minibatch_data = minibatchData(x_valid1, y_valid1)\n",
    "valid2_minibatch_data = minibatchData(x_valid2, y_valid2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining weight_variables and bias_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variables(shape):\n",
    "    initial = tf.ones(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variables(shape):\n",
    "    initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining conv2d and maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = 'SAME')\n",
    "\n",
    "def maxpool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None,784])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "y = tf.placeholder(\"int64\", [None,])\n",
    "y_dummies = tf.one_hot(y,depth = 10)\n",
    "\n",
    "drop_prob = tf.placeholder(\"float\")\n",
    "training = tf.placeholder(\"bool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_w = weight_variables([5,5,1,128])\n",
    "l1_b = bias_variables([128])\n",
    "l1_conv = conv2d(x_image, l1_w) + l1_b\n",
    "l1_relu = tf.nn.relu(l1_conv)\n",
    "l1_maxpool = maxpool(l1_relu)\n",
    "l1_dropout = tf.layers.dropout(l1_maxpool,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_w = weight_variables([5,5,128,256])\n",
    "l2_conv = conv2d(l1_dropout, l2_w)\n",
    "l2_batch_normalization = tf.layers.batch_normalization(l2_conv)\n",
    "l2_leaky_relu = tf.nn.leaky_relu(l2_batch_normalization)\n",
    "l2_maxpool = maxpool(l2_leaky_relu)\n",
    "l2_dropout = tf.layers.dropout(l2_maxpool,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_w = weight_variables([5,5,256,384])\n",
    "l3_conv = conv2d(l2_dropout, l3_w)\n",
    "l3_batch_normalization = tf.layers.batch_normalization(l3_conv)\n",
    "l3_leaky_relu = tf.nn.leaky_relu(l3_batch_normalization)\n",
    "l3_maxpool = maxpool(l3_leaky_relu)\n",
    "l3_dropout = tf.layers.dropout(l3_maxpool,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_w = weight_variables([5,5,384,512])\n",
    "l4_conv = conv2d(l3_dropout, l4_w)\n",
    "l4_batch_normalization = tf.layers.batch_normalization(l4_conv)\n",
    "l4_leaky_relu = tf.nn.leaky_relu(l4_batch_normalization)\n",
    "l4_maxpool = maxpool(l4_leaky_relu)\n",
    "l4_dropout = tf.layers.dropout(l4_maxpool,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_reshape = tf.reshape(l4_dropout,[-1,2048])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "l5_w = weight_variables([2048,512])\n",
    "l5_batch_normalization = tf.layers.batch_normalization(l4_reshape, training = training)\n",
    "l5_inner_product = tf.matmul(l5_batch_normalization, l5_w)\n",
    "l5_leaky_relu = tf.nn.leaky_relu(l5_inner_product)\n",
    "l5_dropout = tf.layers.dropout(l5_leaky_relu,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "l6_w = weight_variables([512,128])\n",
    "l6_batch_normalization = tf.layers.batch_normalization(l5_dropout, training = training)\n",
    "l6_inner_product = tf.matmul(l6_batch_normalization, l6_w)\n",
    "l6_leaky_relu = tf.nn.leaky_relu(l6_inner_product)\n",
    "l6_dropout = tf.layers.dropout(l6_leaky_relu,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "l7_w = weight_variables([128,10])\n",
    "l7_b = bias_variables([10])\n",
    "l7_batch_normalization =  tf.layers.batch_normalization(l6_dropout, training = training)\n",
    "l7_inner_product = tf.matmul(l7_batch_normalization, l7_w) + l7_b\n",
    "l7_log_softmax = tf.nn.log_softmax(l7_inner_product)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_loss = -tf.reduce_mean( tf.multiply(y_dummies,l7_log_softmax) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = tf.argmax(l7_log_softmax,axis=1)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(y, pred_labels),\"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.placeholder(\"float\")\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(xent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0  max_valid_acc = 0.10259999979287386\n",
      "epoch : 0 -- training cross-entropy : 0.20075667399913072\n",
      "epoch : 0 training_acc = 0.09974999983329326 valid_acc = 0.10259999979287386\n",
      "epoch : 1  max_valid_acc = 0.10259999979287386\n",
      "epoch : 2  max_valid_acc = 0.10409999992698431\n",
      "epoch : 3  max_valid_acc = 0.3780000001192093\n",
      "epoch : 4  max_valid_acc = 0.4589999982714653\n",
      "epoch : 5  max_valid_acc = 0.5614999955892563\n",
      "epoch : 6  max_valid_acc = 0.6899000024795532\n",
      "epoch : 7  max_valid_acc = 0.7449999970197677\n",
      "epoch : 8  max_valid_acc = 0.7757999992370606\n",
      "epoch : 9  max_valid_acc = 0.8534999972581864\n",
      "epoch : 12  max_valid_acc = 0.8592999982833862\n",
      "lr = 0.08000000000000002  xent : 0.03565693165292032\n",
      "epoch : 13  max_valid_acc = 0.8762999981641769\n",
      "epoch : 15  max_valid_acc = 0.8802999979257584\n",
      "epoch : 17  max_valid_acc = 0.8852000027894974\n",
      "epoch : 20  max_valid_acc = 0.9095000016689301\n",
      "epoch : 24  max_valid_acc = 0.9109999984502792\n",
      "epoch : 25 -- training cross-entropy : 0.022645503322128207\n",
      "lr = 0.06400000000000002  xent : 0.02361496598343365\n",
      "epoch : 30  max_valid_acc = 0.9148000025749207\n",
      "lr = 0.051200000000000016  xent : 0.017863991054473446\n",
      "epoch : 35  max_valid_acc = 0.917300003170967\n",
      "epoch : 36  max_valid_acc = 0.9179000008106232\n",
      "epoch : 38  max_valid_acc = 0.9212000036239624\n",
      "lr = 0.04096000000000002  xent : 0.013769420259050093\n",
      "epoch : 49  max_valid_acc = 0.9212000048160554\n",
      "epoch : 50 -- training cross-entropy : 0.010140825781563763\n",
      "epoch : 50 training_acc = 0.9763250112533569 valid_acc = 0.9199000012874603\n",
      "epoch : 60  max_valid_acc = 0.9253000032901764\n",
      "lr = 0.03276800000000001  xent : 0.008521717449621064\n",
      "epoch : 71  max_valid_acc = 0.928300005197525\n",
      "lr = 0.026214400000000013  xent : 0.006885704502856242\n",
      "epoch : 75 -- training cross-entropy : 0.005803987135550415\n",
      "lr = 0.02097152000000001  xent : 0.005492067140767176\n",
      "lr = 0.016777216000000008  xent : 0.004501868700172054\n",
      "lr = 0.013421772800000007  xent : 0.0037822292225791897\n",
      "lr = 0.010737418240000006  xent : 0.002957407865812911\n",
      "epoch : 100 -- training cross-entropy : 0.0026804600802142887\n",
      "epoch : 100 training_acc = 0.9970250026881695 valid_acc = 0.9233000046014785\n",
      "lr = 0.008589934592000005  xent : 0.002598543314024937\n",
      "lr = 0.0068719476736000045  xent : 0.002345159877008882\n",
      "epoch : 110  max_valid_acc = 0.9289000058174133\n",
      "lr = 0.005497558138880004  xent : 0.002262152568653164\n",
      "epoch : 113  max_valid_acc = 0.9294000029563904\n",
      "lr = 0.004398046511104004  xent : 0.0021365167253588877\n",
      "lr = 0.0035184372088832034  xent : 0.0018301579984608906\n",
      "lr = 0.002814749767106563  xent : 0.0017783810942137279\n",
      "epoch : 125 -- training cross-entropy : 0.0017783810942137279\n",
      "lr = 0.0022517998136852503  xent : 0.0017606086026614775\n",
      "lr = 0.0018014398509482003  xent : 0.0017444020037714835\n",
      "lr = 0.0014411518807585604  xent : 0.0015945253956761007\n",
      "lr = 0.0011529215046068484  xent : 0.0014868683625991253\n",
      "lr = 0.0009223372036854787  xent : 0.00145868183858056\n",
      "lr = 0.000737869762948383  xent : 0.0014721453414495045\n",
      "lr = 0.0005902958103587065  xent : 0.001390531122037828\n",
      "epoch : 150 -- training cross-entropy : 0.001390531122037828\n",
      "epoch : 150 training_acc = 0.9992500007152557 valid_acc = 0.9288000059127808\n",
      "lr = 0.0004722366482869652  xent : 0.0014325732757947663\n",
      "lr = 0.0003777893186295722  xent : 0.0015556905549738076\n",
      "lr = 0.00030223145490365774  xent : 0.0015226106735372015\n",
      "lr = 0.0002417851639229262  xent : 0.0014176806567934364\n",
      "lr = 0.00019342813113834098  xent : 0.0013882737824644665\n",
      "lr = 0.0001547425049106728  xent : 0.0013047830379809965\n",
      "lr = 0.00012379400392853823  xent : 0.0014439281688646587\n",
      "lr = 9.903520314283059e-05  xent : 0.00150134801268905\n",
      "lr = 7.922816251426448e-05  xent : 0.0016220217974228035\n",
      "lr = 6.338253001141159e-05  xent : 0.0014218983716762067\n",
      "lr = 5.070602400912927e-05  xent : 0.001508296284973767\n",
      "epoch : 175 -- training cross-entropy : 0.001362747511973339\n",
      "lr = 4.056481920730342e-05  xent : 0.001394768448328705\n",
      "lr = 3.2451855365842736e-05  xent : 0.001428057772802731\n",
      "lr = 2.596148429267419e-05  xent : 0.001468557970549682\n",
      "lr = 2.0769187434139353e-05  xent : 0.0014612619799981984\n",
      "lr = 1.6615349947311485e-05  xent : 0.0014100625243600006\n",
      "lr = 1.3292279957849188e-05  xent : 0.0013132487350367228\n",
      "lr = 1.0633823966279351e-05  xent : 0.0013780221626666389\n",
      "lr = 8.507059173023481e-06  xent : 0.0013248859662473933\n",
      "lr = 6.805647338418785e-06  xent : 0.001409680477699453\n",
      "lr = 5.444517870735028e-06  xent : 0.0014328501948671147\n",
      "epoch : 200 -- training cross-entropy : 0.0014168792848823842\n",
      "epoch : 200 training_acc = 0.9993000006675721 valid_acc = 0.9286000061035157\n",
      "lr = 4.355614296588023e-06  xent : 0.0013384510623541245\n",
      "lr = 3.4844914372704182e-06  xent : 0.0014873206860511346\n",
      "lr = 2.7875931498163346e-06  xent : 0.0013232601313529812\n",
      "lr = 2.2300745198530677e-06  xent : 0.0013493031188079386\n",
      "lr = 1.7840596158824543e-06  xent : 0.001393083059183482\n",
      "lr = 1.4272476927059635e-06  xent : 0.0015550262260239833\n",
      "lr = 1.1417981541647709e-06  xent : 0.0014305191464302424\n",
      "epoch : 225 -- training cross-entropy : 0.0014305191464302424\n",
      "lr = 9.134385233318168e-07  xent : 0.0014554086721386738\n",
      "lr = 7.307508186654535e-07  xent : 0.0014332139946827737\n",
      "lr = 5.846006549323629e-07  xent : 0.001461299059811836\n",
      "lr = 4.6768052394589033e-07  xent : 0.0015013669670088348\n",
      "lr = 3.741444191567123e-07  xent : 0.0013267309332900368\n",
      "lr = 2.993155353253699e-07  xent : 0.001323605564319905\n",
      "lr = 2.3945242826029593e-07  xent : 0.0013727072634628712\n",
      "lr = 1.9156194260823676e-07  xent : 0.0014031697506061392\n",
      "lr = 1.5324955408658942e-07  xent : 0.0013917243753667208\n",
      "lr = 1.2259964326927154e-07  xent : 0.0014808202835888552\n",
      "epoch : 250 -- training cross-entropy : 0.0014808202835888552\n",
      "epoch : 250 training_acc = 0.9993000006675721 valid_acc = 0.9287000060081482\n",
      "lr = 9.807971461541724e-08  xent : 0.0015184398008864263\n",
      "lr = 7.84637716923338e-08  xent : 0.0013454636875849246\n",
      "lr = 6.277101735386704e-08  xent : 0.0014081707230320718\n",
      "lr = 5.021681388309364e-08  xent : 0.0013018780197745626\n",
      "lr = 4.0173451106474913e-08  xent : 0.0013804812168450554\n",
      "lr = 3.2138760885179933e-08  xent : 0.001397232373259385\n",
      "lr = 2.571100870814395e-08  xent : 0.0013858371233891375\n",
      "epoch : 275 -- training cross-entropy : 0.0014286306433473328\n",
      "lr = 2.056880696651516e-08  xent : 0.00143079113973954\n",
      "lr = 1.645504557321213e-08  xent : 0.0014034495434361817\n",
      "lr = 1.3164036458569704e-08  xent : 0.0013823974965248454\n",
      "lr = 1.0531229166855763e-08  xent : 0.001307284185759272\n",
      "lr = 8.424983333484611e-09  xent : 0.0013358738193733188\n",
      "lr = 6.7399866667876894e-09  xent : 0.0015381706934215345\n",
      "lr = 5.3919893334301516e-09  xent : 0.0014980570240572888\n",
      "lr = 4.313591466744121e-09  xent : 0.0014300877286001423\n",
      "epoch : 300 -- training cross-entropy : 0.0013361721063074583\n",
      "epoch : 300 training_acc = 0.9993000006675721 valid_acc = 0.9286000061035157\n",
      "lr = 3.450873173395297e-09  xent : 0.001374402545815201\n",
      "lr = 2.760698538716238e-09  xent : 0.0014554639539471736\n",
      "lr = 2.2085588309729903e-09  xent : 0.0014239737042680645\n",
      "lr = 1.7668470647783923e-09  xent : 0.0013036440635266898\n",
      "lr = 1.413477651822714e-09  xent : 0.0014933586363167705\n",
      "lr = 1.1307821214581714e-09  xent : 0.001362507908305588\n",
      "lr = 9.046256971665372e-10  xent : 0.001345826182375731\n",
      "lr = 7.237005577332297e-10  xent : 0.0013570179108910452\n",
      "lr = 5.789604461865838e-10  xent : 0.001466284166724563\n",
      "lr = 4.6316835694926706e-10  xent : 0.001421633920753038\n",
      "epoch : 325 -- training cross-entropy : 0.0012821384699111605\n",
      "lr = 3.7053468555941366e-10  xent : 0.0013860450981610483\n",
      "lr = 2.9642774844753097e-10  xent : 0.0014772587759455291\n",
      "lr = 2.371421987580248e-10  xent : 0.0015844202021798991\n",
      "lr = 1.8971375900641987e-10  xent : 0.0013796005230187801\n",
      "lr = 1.517710072051359e-10  xent : 0.0014675168331780243\n",
      "lr = 1.2141680576410873e-10  xent : 0.001430067076978503\n",
      "lr = 9.713344461128699e-11  xent : 0.0012514279435833942\n",
      "lr = 7.77067556890296e-11  xent : 0.0015704663854762657\n",
      "lr = 6.216540455122369e-11  xent : 0.0013553342521458944\n",
      "epoch : 350 -- training cross-entropy : 0.001372478183950534\n",
      "epoch : 350 training_acc = 0.9993000006675721 valid_acc = 0.9287000060081482\n",
      "lr = 4.973232364097895e-11  xent : 0.001291126136516141\n",
      "lr = 3.978585891278316e-11  xent : 0.0014580830576289827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr = 3.182868713022653e-11  xent : 0.001512122684066526\n",
      "lr = 2.546294970418123e-11  xent : 0.0013737875058950522\n",
      "lr = 2.0370359763344985e-11  xent : 0.001426410239823781\n",
      "lr = 1.6296287810675988e-11  xent : 0.0014247645821228617\n",
      "lr = 1.3037030248540791e-11  xent : 0.001381026513922734\n",
      "epoch : 375 -- training cross-entropy : 0.0013770572810744852\n",
      "lr = 1.0429624198832634e-11  xent : 0.0013906603175314557\n",
      "lr = 8.343699359066108e-12  xent : 0.0013383374376064695\n",
      "lr = 6.6749594872528864e-12  xent : 0.0014449426175184498\n",
      "lr = 5.339967589802309e-12  xent : 0.001354550218929944\n",
      "lr = 4.271974071841848e-12  xent : 0.0013296149010511727\n",
      "lr = 3.4175792574734783e-12  xent : 0.0012687684504498974\n",
      "lr = 2.734063405978783e-12  xent : 0.0015281811100305732\n",
      "lr = 2.1872507247830263e-12  xent : 0.0013466358766333997\n",
      "epoch : 400 -- training cross-entropy : 0.0013466358766333997\n",
      "epoch : 400 training_acc = 0.9993000006675721 valid_acc = 0.9287000060081482\n"
     ]
    }
   ],
   "source": [
    "epochs = 401\n",
    "batch_size = 100\n",
    "\n",
    "tmp_xent_loss_3 = [1.0,1.0,1.0]\n",
    "learning_rate = 0.1\n",
    "rep_num = int((x_train.shape[0])/batch_size)\n",
    "max_valid1_acc = .0\n",
    "valid1_rep_num = int((x_valid1.shape[0])/batch_size)\n",
    "\n",
    "for i in range(epochs):\n",
    "    tmp_loss_vec = [.0 for a in range(rep_num)]\n",
    "    tmp_valid1_acc_vec = [.0 for a in range(valid1_rep_num)]\n",
    "    tmp_train_acc_vec = [.0 for a in range(rep_num)]\n",
    "    for j in range(rep_num):\n",
    "        batch_train_x, batch_train_y = train_minibatch_data.minibatch(batch_size)\n",
    "        feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : 3/8, training : True, lr : learning_rate}\n",
    "        _, tmp_loss_vec[j] = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    tmp_xent_loss_3 = [tmp_xent_loss_3[1], tmp_xent_loss_3[2], sum(tmp_loss_vec)/rep_num]\n",
    "   \n",
    "    if tmp_xent_loss_3[0] == min(tmp_xent_loss_3):\n",
    "        learning_rate = learning_rate * .8\n",
    "        print(\"lr = \" + str(learning_rate) + \"  xent : \" + str(tmp_xent_loss_3[2]))\n",
    "\n",
    "    for j in range(valid1_rep_num):\n",
    "        batch_valid1_x, batch_valid1_y = valid1_minibatch_data.minibatch(batch_size)\n",
    "        feed_dict = {x : batch_valid1_x, y : batch_valid1_y, drop_prob : 3/8, training : False}\n",
    "        tmp_valid1_acc_vec[j] = sess.run(acc, feed_dict = feed_dict)\n",
    "\n",
    "    valid1_acc = sum(tmp_valid1_acc_vec)/valid1_rep_num\n",
    "    \n",
    "    if valid1_acc >= max_valid1_acc:\n",
    "        max_valid1_acc = valid1_acc\n",
    "        print(\"epoch : \" + str(i) + \"  max_valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./CNN5/model.ckpt\")\n",
    "        \n",
    "    if i % 25 == 0:\n",
    "        print(\"epoch : \" + str(i) + \" -- training cross-entropy : \" + str(tmp_xent_loss_3[2]))\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        for j in range(rep_num):\n",
    "            batch_train_x, batch_train_y = train_minibatch_data.minibatch(batch_size)\n",
    "            feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : 3/8, training : False}\n",
    "            tmp_train_acc_vec[j] = sess.run(acc, feed_dict = feed_dict)\n",
    "            \n",
    "        train_acc = sum(tmp_train_acc_vec)/rep_num\n",
    "        print(\"epoch : \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./CNN5/model.ckpt\n",
      "Model restored.\n"
     ]
    }
   ],
   "source": [
    "saver.restore(sess, \"./CNN5/model.ckpt\")\n",
    "print(\"Model restored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "rep_num = int((x_train.shape[0])/batch_size)\n",
    "tmp_train_acc_vec = [.0 for a in range(rep_num)]\n",
    "CNN5_predict_train = []\n",
    "\n",
    "for j in range(rep_num):\n",
    "    batch_train_x, batch_train_y = train_minibatch_data.minibatch(batch_size)\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : 3/8, training : False}\n",
    "    tmp_CNN5_predict_train, tmp_train_acc_vec[j] = sess.run([pred_labels,acc], feed_dict = feed_dict)\n",
    "    CNN5_predict_train = np.concatenate([CNN5_predict_train, tmp_CNN5_predict_train])\n",
    "\n",
    "CNN5_train_acc = sum(tmp_train_acc_vec)/rep_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3990    0    0    0    0    0    1    0    0    0]\n",
      " [   1 3990    3    2    1    0    4    0    0    0]\n",
      " [   0    0 4053    0    0    0    5    0    0    0]\n",
      " [   0    0    0 3926    0    0    0    0    0    0]\n",
      " [   0    0    0    0 4015    0    3    0    0    0]\n",
      " [   1    0    0    1    0 3932    0    0    0    0]\n",
      " [   1    0    0    0    0    0 3992    0    0    0]\n",
      " [   0    0    0    0    0    0    0 4096    0    0]\n",
      " [   0    0    0    0    0    0    0    0 3946    0]\n",
      " [   1    0    0    0    0    0    0    7    0 4029]]\n",
      "TRAINING ACCURACY = 0.9992250084877015\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(CNN5_predict_train,y_train))\n",
    "print(\"TRAINING ACCURACY =\",CNN5_train_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "valid1_rep_num = int((x_valid1.shape[0])/batch_size)\n",
    "tmp_valid1_acc_vec = [.0 for a in range(rep_num)]\n",
    "CNN5_predict_valid1 = []\n",
    "\n",
    "for j in range(valid1_rep_num):\n",
    "    batch_valid1_x, batch_valid1_y = valid1_minibatch_data.minibatch(batch_size)\n",
    "    feed_dict = {x : batch_valid1_x, y : batch_valid1_y, drop_prob : 3/8, training : False}\n",
    "    tmp_CNN5_predict_valid1, tmp_valid1_acc_vec[j] = sess.run([pred_labels,acc], feed_dict = feed_dict)\n",
    "    CNN5_predict_valid1 = np.concatenate([CNN5_predict_valid1, tmp_CNN5_predict_valid1])\n",
    "\n",
    "CNN5_valid1_acc = sum(tmp_valid1_acc_vec)/valid1_rep_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 904    0   13   10    0    0   79    0    2    0]\n",
      " [   7 1025    0   17    7    0    8    0    0    0]\n",
      " [   8    0  848    2   37    0   81    0    0    0]\n",
      " [  23    1    9  953   33    0   18    0    1    0]\n",
      " [   1    0   54   10  890    0   64    0    0    0]\n",
      " [   0    0    0    1    1 1055    0   11    2    5]\n",
      " [  67    0   21   10   23    0  726    0    3    0]\n",
      " [   0    0    0    0    0    2    0  908    0   14]\n",
      " [   5    0    0    9    4    1   11    0 1026    0]\n",
      " [   0    0    0    0    0    2    0   29    0  959]]\n",
      "VALIDATION ACCURACY = 0.9293999969959259\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(CNN5_predict_valid1,y_valid1))\n",
    "print(\"VALIDATION ACCURACY =\",CNN5_valid1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN_ACC': 0.9992250084877015, 'VALID_ACC': 0.9293999969959259}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"TRAIN_ACC\" : CNN5_train_acc , \"VALID_ACC\" : CNN5_valid1_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
