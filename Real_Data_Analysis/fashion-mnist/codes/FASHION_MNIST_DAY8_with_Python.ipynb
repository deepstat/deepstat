{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASHION MNIST with Python (DAY 8)\n",
    "\n",
    "DATA SOURCE : https://www.kaggle.com/zalando-research/fashionmnist (Kaggle, Fashion MNIST)\n",
    "\n",
    "FASHION MNIST with Python (DAY 1) : http://deepstat.tistory.com/35\n",
    "\n",
    "FASHION MNIST with Python (DAY 2) : http://deepstat.tistory.com/36\n",
    "\n",
    "FASHION MNIST with Python (DAY 3) : http://deepstat.tistory.com/37\n",
    "\n",
    "FASHION MNIST with Python (DAY 4) : http://deepstat.tistory.com/38\n",
    "\n",
    "FASHION MNIST with Python (DAY 5) : http://deepstat.tistory.com/39\n",
    "\n",
    "FASHION MNIST with Python (DAY 6) : http://deepstat.tistory.com/40\n",
    "\n",
    "FASHION MNIST with Python (DAY 7) : http://deepstat.tistory.com/41"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing numpy, pandas, pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../datasets/fashion-mnist_train.csv\")\n",
    "data_test = pd.read_csv(\"../datasets/fashion-mnist_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_train_y = data_train.label\n",
    "y_test = data_test.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train_x = data_train.drop(\"label\",axis=1)/256\n",
    "x_test = data_test.drop(\"label\",axis=1)/256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting valid and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "valid2_idx = np.random.choice(60000,10000,replace = False)\n",
    "valid1_idx = np.random.choice(list(set(range(60000)) - set(valid2_idx)),10000,replace=False)\n",
    "train_idx = list(set(range(60000))-set(valid1_idx)-set(valid2_idx))\n",
    "\n",
    "x_train = data_train_x.iloc[train_idx,:]\n",
    "y_train = data_train_y.iloc[train_idx]\n",
    "\n",
    "x_valid1 = data_train_x.iloc[valid1_idx,:]\n",
    "y_valid1 = data_train_y.iloc[valid1_idx]\n",
    "\n",
    "x_valid2 = data_train_x.iloc[valid2_idx,:]\n",
    "y_valid2 = data_train_y.iloc[valid2_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining weight_variables and bias_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_variables(shape):\n",
    "    initial = tf.truncated_normal(shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variables(shape):\n",
    "    initial = tf.zeros(shape)\n",
    "    return tf.Variable(initial)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining conv2d and maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv2d(x,W):\n",
    "    return tf.nn.conv2d(x, W, strides = [1, 1, 1, 1], padding = 'VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxpool(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides = [1, 2, 2, 1], padding = 'VALID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the CNN\n",
    "\n",
    "Convolution, Maxout, Maxpooling, Dropout, Softmax, Cross-Entropy, Adam\n",
    "\n",
    "- Model : input -> [convolution -> maxout -> dropout] -> [convolution -> batch normalizaton -> maxout -> maxpool -> dropout] -> [convolution -> batch normalizaton -> maxout -> dropout] -> flatten -> [batch normalization -> inner product -> softmax] -> output\n",
    "\n",
    "- Loss : cross entropy\n",
    "\n",
    "- Optimizer : Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None,784])\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "y = tf.placeholder(\"int64\", [None,])\n",
    "y_dummies = tf.one_hot(y,depth = 10)\n",
    "\n",
    "drop_prob = tf.placeholder(\"float\")\n",
    "training = tf.placeholder(\"bool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1_w = weight_variables([5,5,1,4*8])\n",
    "l1_b = bias_variables([4*8])\n",
    "l1_conv = conv2d(x_image, l1_w) + l1_b\n",
    "l1_maxout = tf.contrib.layers.maxout(l1_conv,8)\n",
    "l1_dropout = tf.layers.dropout(l1_maxout,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_w = weight_variables([5,5,8,4*8])\n",
    "l2_conv = conv2d(l1_dropout, l2_w)\n",
    "l2_batch_normalization = tf.layers.batch_normalization(l2_conv)\n",
    "l2_maxout = tf.contrib.layers.maxout(l2_batch_normalization,8)\n",
    "l2_maxpool = maxpool(l2_maxout)\n",
    "l2_dropout = tf.layers.dropout(l2_maxpool,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_w = weight_variables([5,5,8,8*4])\n",
    "l3_conv = conv2d(l2_dropout, l3_w)\n",
    "l3_batch_normalization = tf.layers.batch_normalization(l3_conv)\n",
    "l3_maxout = tf.contrib.layers.maxout(l3_batch_normalization,4)\n",
    "l3_dropout = tf.layers.dropout(l3_maxout,rate = drop_prob, training = training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3_reshape = tf.reshape(l3_dropout,[-1,6*6*4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "l4_w = weight_variables([6*6*4,10])\n",
    "l4_b = bias_variables([10])\n",
    "l4_batch_normalization = tf.layers.batch_normalization(l3_reshape)\n",
    "l4_inner_prod = tf.matmul(l4_batch_normalization, l4_w) + l4_b\n",
    "l4_log_softmax = tf.nn.log_softmax(l4_inner_prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "xent_loss = -tf.reduce_sum( tf.multiply(y_dummies,l4_log_softmax) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = tf.argmax(l4_log_softmax,axis=1)\n",
    "acc = tf.reduce_mean(tf.cast(tf.equal(y, pred_labels),\"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = tf.placeholder(\"float\")\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(xent_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 42244024.0\n",
      "step 0 training_acc = 0.074 valid_acc = 0.0896\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 2000 training cross-entropy : 907.0232\n",
      "step 4000 training cross-entropy : 834394500.0\n",
      "step 4000 training_acc = 0.178 valid_acc = 0.1693\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 6000 training cross-entropy : 881304.6\n",
      "step 8000 training cross-entropy : 108451.55\n",
      "step 8000 training_acc = 0.195 valid_acc = 0.19\n",
      "Model saved in path: ./CNN/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "for i in range(8001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : True, lr : 0.1}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 2000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 4000 == 0:\n",
    "        feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./CNN/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 266021.25\n",
      "step 0 training_acc = 0.203 valid_acc = 0.2017\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 4000 training cross-entropy : 53425.797\n",
      "step 8000 training cross-entropy : 13400.107\n",
      "step 8000 training_acc = 0.493 valid_acc = 0.4964\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 12000 training cross-entropy : 1908.7681\n",
      "step 16000 training cross-entropy : 1130.8694\n",
      "step 16000 training_acc = 0.609 valid_acc = 0.5841\n",
      "Model saved in path: ./CNN/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "for i in range(16001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : True, lr : 0.01}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 4000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 8000 == 0:\n",
    "        feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./CNN/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 1238.7634\n",
      "step 0 training_acc = 0.619 valid_acc = 0.5912\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 20000 training cross-entropy : 619.7753\n",
      "step 40000 training cross-entropy : 487.32724\n",
      "step 40000 training_acc = 0.849 valid_acc = 0.8291\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 60000 training cross-entropy : 435.53052\n",
      "step 80000 training cross-entropy : 411.05118\n",
      "step 80000 training_acc = 0.885 valid_acc = 0.8694\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 100000 training cross-entropy : 371.35452\n",
      "step 120000 training cross-entropy : 374.37384\n",
      "step 120000 training_acc = 0.896 valid_acc = 0.8806\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 140000 training cross-entropy : 352.6808\n",
      "step 160000 training cross-entropy : 323.04425\n",
      "step 160000 training_acc = 0.904 valid_acc = 0.8881\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 180000 training cross-entropy : 339.18182\n",
      "step 200000 training cross-entropy : 328.336\n",
      "step 200000 training_acc = 0.908 valid_acc = 0.8894\n",
      "Model saved in path: ./CNN/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "for i in range(200001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : True, lr : 0.001}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 20000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 40000 == 0:\n",
    "        feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./CNN/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 training cross-entropy : 290.74316\n",
      "step 0 training_acc = 0.91 valid_acc = 0.8893\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 40000 training cross-entropy : 295.59088\n",
      "step 80000 training cross-entropy : 317.6109\n",
      "step 80000 training_acc = 0.919 valid_acc = 0.8934\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 120000 training cross-entropy : 279.0041\n",
      "step 160000 training cross-entropy : 309.32037\n",
      "step 160000 training_acc = 0.905 valid_acc = 0.8947\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 200000 training cross-entropy : 303.6273\n",
      "step 240000 training cross-entropy : 297.97955\n",
      "step 240000 training_acc = 0.908 valid_acc = 0.8961\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 280000 training cross-entropy : 280.16525\n",
      "step 320000 training cross-entropy : 314.53802\n",
      "step 320000 training_acc = 0.904 valid_acc = 0.8962\n",
      "Model saved in path: ./CNN/model.ckpt\n",
      "step 360000 training cross-entropy : 307.3462\n",
      "step 400000 training cross-entropy : 291.04474\n",
      "step 400000 training_acc = 0.925 valid_acc = 0.8963\n",
      "Model saved in path: ./CNN/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "for i in range(400001):\n",
    "    batch_obs = np.random.choice(x_train.shape[0],batch_size,replace=False)\n",
    "    batch_train_x = x_train.iloc[batch_obs]\n",
    "    batch_train_y = y_train.iloc[batch_obs]\n",
    "    feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : True, lr : 0.0001}\n",
    "    _, tmp = sess.run([train_step,xent_loss], feed_dict = feed_dict)\n",
    "    \n",
    "    if i % 40000 == 0:\n",
    "        print(\"step \" + str(i) + \" training cross-entropy : \" + str(tmp))\n",
    "    \n",
    "    if i % 80000 == 0:\n",
    "        feed_dict = {x : batch_train_x, y : batch_train_y, drop_prob : .125, training : False}\n",
    "        train_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "        valid1_acc = sess.run(acc, feed_dict = feed_dict)\n",
    "        print(\"step \" + str(i) + \" training_acc = \" + str(train_acc) + \" valid_acc = \" + str(valid1_acc))\n",
    "        save_path = saver.save(sess, \"./CNN/model.ckpt\")\n",
    "        print(\"Model saved in path: \" + save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict1 = {x : x_train.iloc[0:8000,], y : y_train[0:8000], drop_prob : .125, training : False}\n",
    "MLP_predict_train1, MLP_train_acc1 = sess.run([pred_labels,acc], feed_dict = feed_dict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict2 = {x : x_train.iloc[8000:16000], y : y_train[8000:16000], drop_prob : .125, training : False}\n",
    "MLP_predict_train2, MLP_train_acc2 = sess.run([pred_labels,acc], feed_dict = feed_dict2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict3 = {x : x_train.iloc[16000:24000], y : y_train[16000:24000], drop_prob : .125, training : False}\n",
    "MLP_predict_train3, MLP_train_acc3 = sess.run([pred_labels,acc], feed_dict = feed_dict3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict4 = {x : x_train.iloc[24000:32000], y : y_train[24000:32000], drop_prob : .125, training : False}\n",
    "MLP_predict_train4, MLP_train_acc4 = sess.run([pred_labels,acc], feed_dict = feed_dict4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict5 = {x : x_train.iloc[32000:40000], y : y_train[32000:40000], drop_prob : .125, training : False}\n",
    "MLP_predict_train5, MLP_train_acc5 = sess.run([pred_labels,acc], feed_dict = feed_dict5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP_predict_train = np.concatenate((MLP_predict_train1, MLP_predict_train2,\n",
    "                                   MLP_predict_train3,MLP_predict_train4,MLP_predict_train5), axis=None)\n",
    "MLP_train_acc = np.mean((MLP_train_acc1,MLP_train_acc2,MLP_train_acc3,MLP_train_acc4,MLP_train_acc5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3500    9   50   55    3    0  455    0    6    0]\n",
      " [   2 3921    2    5    3    0    3    0    1    0]\n",
      " [  75    2 3544   28  177    1  270    0   22    0]\n",
      " [  74   39   26 3628   96    1   87    0    7    1]\n",
      " [  10   10  233   96 3555    0  320    0    9    1]\n",
      " [   0    0    0    0    0 3839    0    6    3    4]\n",
      " [ 310    6  194  115  181    0 2853    0   18    0]\n",
      " [   0    0    0    0    0   64    0 3991    0   67]\n",
      " [  23    3    7    2    1    1   17    4 3880    1]\n",
      " [   0    0    0    0    0   26    0  102    0 3955]]\n",
      "TRAINING ACCURACY = 0.91665\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(MLP_predict_train,y_train))\n",
    "print(\"TRAINING ACCURACY =\",MLP_train_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_dict = {x : x_valid1, y : y_valid1, drop_prob : .125, training : False}\n",
    "MLP_predict_valid1, MLP_valid1_acc = sess.run([pred_labels,acc], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 885    2   13   22    4    0  123    0    7    0]\n",
      " [   1 1004    0    7    1    0    1    0    0    0]\n",
      " [  13    2  784    8   46    0   82    0    4    0]\n",
      " [  27   11    9  921   44    1   21    0    7    0]\n",
      " [   4    1   68   31  826    0   86    0    4    0]\n",
      " [   0    0    0    0    0 1017    0    3    1    3]\n",
      " [  81    6   68   22   71    0  665    0    8    0]\n",
      " [   0    0    0    0    0   25    0  913    1   22]\n",
      " [   4    0    3    1    3    2    9    2 1002    7]\n",
      " [   0    0    0    0    0   15    0   30    0  946]]\n",
      "VALIDATION ACCURACY = 0.8963\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(MLP_predict_valid1,y_valid1))\n",
    "print(\"VALIDATION ACCURACY =\",MLP_valid1_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'TRAIN_ACC': 0.91665, 'VALID_ACC': 0.8963}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"TRAIN_ACC\" : MLP_train_acc , \"VALID_ACC\" : MLP_valid1_acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
